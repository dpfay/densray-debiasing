{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DRBERT_WEAT.ipynb","provenance":[{"file_id":"1h7VL-novxc3HQi7yzDaYbWQxgpWR3Awj","timestamp":1587958740336},{"file_id":"1Cd8veF5937_W3bS0jQ8KI1edeZXRwmVn","timestamp":1587435514487},{"file_id":"https://github.com/jsedoc/ConceptorDebias/blob/master/WEAT/WEAT_(Final).ipynb","timestamp":1587337519818}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9d958c1e732f4dfa8d6cb83d3f5da7e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b0842ba59b34424d93af3a0a3e0e8c57","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_63addb5de5074e66804d5f518b1e82c2","IPY_MODEL_00571cf7d1d946af8fbd845af7f96137"]}},"b0842ba59b34424d93af3a0a3e0e8c57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"63addb5de5074e66804d5f518b1e82c2":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_af755d64bcb24a9fbdbafeaf847c933e","_dom_classes":[],"description":"Downloading: 100%","_model_name":"IntProgressModel","bar_style":"success","max":1344997306,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1344997306,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bc4559be90c9492b9f3ec9fa9ce46831"}},"00571cf7d1d946af8fbd845af7f96137":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c58f5566461947378cc8abccfb9ecbe2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.34G/1.34G [01:57&lt;00:00, 11.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ff276ca0e9734d739a39dc8c85d797b4"}},"af755d64bcb24a9fbdbafeaf847c933e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bc4559be90c9492b9f3ec9fa9ce46831":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c58f5566461947378cc8abccfb9ecbe2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ff276ca0e9734d739a39dc8c85d797b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"477b4625f0124881a2b034d609ad9f66":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8ca3f6faf95c4821801d7330ccd58a12","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_617ad757d8c74e7795f9521ba521e146","IPY_MODEL_b3be7426808e42dbb7cf8248d8d0af9c"]}},"8ca3f6faf95c4821801d7330ccd58a12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"617ad757d8c74e7795f9521ba521e146":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e99d4298688b4d9b976623f90ae2dde1","_dom_classes":[],"description":"Downloading: 100%","_model_name":"IntProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2baba2d6cee549c08e56fd839fcc994d"}},"b3be7426808e42dbb7cf8248d8d0af9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3ed474eb73ab4cf09941d96a8abfa433","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 2.68MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5154d0cac1584f79a2cc8d24690777d9"}},"e99d4298688b4d9b976623f90ae2dde1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2baba2d6cee549c08e56fd839fcc994d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ed474eb73ab4cf09941d96a8abfa433":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5154d0cac1584f79a2cc8d24690777d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"bY4RMFQxrWyv","colab_type":"code","colab":{}},"source":["import numpy as np\n","from itertools import combinations, filterfalse\n","from sklearn.metrics.pairwise import cosine_similarity\n","from gensim.models.keyedvectors import KeyedVectors\n","import pandas as pd\n","import random\n","import sys\n","import os\n","import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kYjBzzsjwEr_","colab_type":"code","outputId":"24a18076-bd0c-4127-9bb0-d1884095510a","executionInfo":{"status":"ok","timestamp":1588244792950,"user_tz":-120,"elapsed":8066,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"colab":{"base_uri":"https://localhost:8080/","height":712}},"source":["!pip install transformers==2.8.0"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers==2.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n","\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 48.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.38.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.21.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 43.8MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.3)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.12.46)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 28.8MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.4.5.1)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.15.46)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.9.5)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers==2.8.0) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers==2.8.0) (2.8.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=4f8a9bf31327e582b50b144216742eea4553019052401423e9a57e608d17b510\n","  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.41 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nka-t9d-Pn7M","colab_type":"code","outputId":"8ef42ccd-ea95-4395-a85a-478ad663051c","executionInfo":{"status":"ok","timestamp":1588244824563,"user_tz":-120,"elapsed":831,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ve3T4fmDyGFN","colab_type":"text"},"source":["# Config"]},{"cell_type":"code","metadata":{"id":"fYFhIYk8v_sL","colab_type":"code","outputId":"ec8d4eb7-bc36-44c1-aaf4-14d4de825b6d","executionInfo":{"status":"ok","timestamp":1588246826612,"user_tz":-120,"elapsed":39668,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9d958c1e732f4dfa8d6cb83d3f5da7e3","b0842ba59b34424d93af3a0a3e0e8c57","63addb5de5074e66804d5f518b1e82c2","00571cf7d1d946af8fbd845af7f96137","af755d64bcb24a9fbdbafeaf847c933e","bc4559be90c9492b9f3ec9fa9ce46831","c58f5566461947378cc8abccfb9ecbe2","ff276ca0e9734d739a39dc8c85d797b4","477b4625f0124881a2b034d609ad9f66","8ca3f6faf95c4821801d7330ccd58a12","617ad757d8c74e7795f9521ba521e146","b3be7426808e42dbb7cf8248d8d0af9c","e99d4298688b4d9b976623f90ae2dde1","2baba2d6cee549c08e56fd839fcc994d","3ed474eb73ab4cf09941d96a8abfa433","5154d0cac1584f79a2cc8d24690777d9"]}},"source":["import torch\n","import transformers\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","config = 'large' #'base'\n","nlayer = 12 if config == 'base' else 24\n","nsamples = 50000\n","\n","model = transformers.BertForMaskedLM.from_pretrained('bert-'+config+'-uncased', output_hidden_states=True).to(device)\n","tokenizer = transformers.BertTokenizer.from_pretrained('bert-'+config+'-uncased')\n","# turn on eval mode\n","model.eval()"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d958c1e732f4dfa8d6cb83d3f5da7e3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=1344997306, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"477b4625f0124881a2b034d609ad9f66","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["BertForMaskedLM(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n","      (position_embeddings): Embedding(512, 1024)\n","      (token_type_embeddings): Embedding(2, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (cls): BertOnlyMLMHead(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (decoder): Linear(in_features=1024, out_features=30522, bias=True)\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"qtb-BE-Y7hId","colab_type":"text"},"source":["## New BERTs"]},{"cell_type":"code","metadata":{"id":"8V7GsHsVbgIt","colab_type":"code","colab":{}},"source":["import densray_bert"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l8Me4_OyFyKo","colab_type":"text"},"source":["# WEAT Algorithm\n","The Word Embeddings Association Test (WEAT), as proposed by Calikson et. al., is a statistical test analogous to the Implicit Association Test (IAT) which helps quantify human biases in textual data. WEAT uses the cosine similarity between word embeddings which is analogous to the reaction time when subjects are asked to pair two concepts they find similar in the IAT.  WEAT considers two sets of target words and two sets of attribute words of equal size. The null hypothesis is that there is no difference between the two sets of target words and the sets of attribute words in terms of their relative similarities measured as the cosine similarity between the embeddings. For example, consider the target sets as words representing *Career* and *Family* and let the two sets of attribute words be *Male* and *Female* in that order. The null hypothesis states that *Career* and *Family* are equally similar (mathematically, in terms of the mean cosine similarity between the word representations) to each of the words in the *Male* and *Female* word lists. \n","\n","REF: https://gist.github.com/SandyRogers/e5c2e938502a75dcae25216e4fae2da5\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oQFPEITekNnV","colab_type":"text"},"source":["## Test Statistic\n","\n","The WEAT test statistic measures the differential association of the two sets of target words with the attribute.\n","\n","To ground this, we cast WEAT in our formulation where $\\mathcal{X}$ and $\\mathcal{Y}$ are two sets of target\n","words, (concretely, $\\mathcal{X}$ might be*Career* words and $\\mathcal{Y}$ *Family* words) and $\\mathcal{A}$, $\\mathcal{B}$ are two sets of attribute words ($\\mathcal{A}$ might be ''female'' names and $\\mathcal{B}$  ''male'' names) assumed to associate with the bias concept(s). WEAT is then\n","\\begin{align*}\n","s(\\mathcal{X}, &\\mathcal{Y}, \\mathcal{A}, \\mathcal{B}) \\\\ &= \\frac{1}{|\\mathcal{X}|}\\Bigg[\\sum_{x \\in \\mathcal{X}}{\\Big[\\sum_{a\\in \\mathcal{A}}{s(x,a)} - \\sum_{b\\in \\mathcal{B}}{s(x,b)}\\Big]} \\\\ &\\hbox{}  - \\sum_{y \\in \\mathcal{Y}}{\\Big[\\sum_{a\\in \\mathcal{A}}{s(y,a)} - \\sum_{b\\in \\mathcal{B}}{s(y,b)}\\Big]}\\Bigg],\n","\\end{align*}\n","where $s(x,y) = \\cos(\\hbox{vec}(x), \\hbox{vec}(y))$ and $\\hbox{vec}(x) \\in \\mathbb{R}^k$ is the $k$-dimensional word embedding for word $x$. We assume that there is no overlap between any of the sets $\\mathcal{X}$, $\\mathcal{Y}$, $\\mathcal{A}$, and $\\mathcal{B}$.\n","\n","Note that for this definition of WEAT, the cardinality of the sets must be equal, so $|\\mathcal{A}|=|\\mathcal{B}|$ and $|\\mathcal{X}|=|\\mathcal{Y}|$. Our  conceptor formulation given below relaxes this assumption."]},{"cell_type":"code","metadata":{"id":"mf6_liysF8en","colab_type":"code","colab":{}},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","def swAB(W, A, B):\n","    \"\"\"Calculates differential cosine-similarity between word vectors in W, A and W, B\n","        Arguments\n","                W, A, B : n x d matrix of word embeddings stored row wise\n","    \"\"\"\n","    WA = cosine_similarity(W,A)\n","    WB = cosine_similarity(W,B)\n","    \n","    #Take mean along columns\n","    WAmean = np.mean(WA, axis = 1)\n","    WBmean = np.mean(WB, axis = 1)\n","    \n","    return (WAmean - WBmean)\n","  \n","def test_statistic(X, Y, A, B):\n","    \"\"\"Calculates test-statistic between the pair of association words and target words\n","        Arguments\n","                X, Y, A, B : n x d matrix of word embeddings stored row wise\n","        Returns\n","                Test Statistic\n","    \"\"\"\n","    return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m9ZO4--vpBh6","colab_type":"text"},"source":["## Effect Size (d-value)\n","\n","The ''effect size'' is a normalized measure of how separated the two distributions are."]},{"cell_type":"code","metadata":{"id":"EHPKwHLjo-m8","colab_type":"code","colab":{}},"source":["def weat_effect_size(X, Y, A, B, embd):\n","    \"\"\"Computes the effect size for the given list of association and target word pairs\n","        Arguments\n","                X, Y : List of association words\n","                A, B : List of target words\n","                embd : Dictonary of word-to-embedding for all words\n","        Returns\n","                Effect Size\n","    \"\"\"\n","    Xmat = np.array([embd[w] for w in X if w in embd])\n","    Ymat = np.array([embd[w] for w in Y if w in embd])\n","    Amat = np.array([embd[w] for w in A if w in embd])\n","    Bmat = np.array([embd[w] for w in B if w in embd])\n","    XuY = list(set(X).union(Y))\n","    XuYmat = []\n","    for w in XuY:\n","        if w.lower() in embd:\n","            XuYmat.append(embd[w.lower()])\n","    XuYmat = np.array(XuYmat)\n","    d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n","    return d"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d8foGwVSGI16","colab_type":"text"},"source":["## P-Value\n","\n","The one-sided P value measures the likelihood that a random permutation of the attribute words would produce at least the observed test statistic"]},{"cell_type":"code","metadata":{"id":"ZDy-duFOFj71","colab_type":"code","colab":{}},"source":["def random_permutation(iterable, r=None):\n","    \"\"\"Returns a random permutation for any iterable object\"\"\"\n","    pool = tuple(iterable)\n","    r = len(pool) if r is None else r\n","    return tuple(random.sample(pool, r))\n","\n","def weat_p_value(X, Y, A, B, embd, sample=None):\n","    np.random.seed(42)\n","    random.seed(42)\n","    \"\"\"Computes the one-sided P value for the given list of association and target word pairs\n","        Arguments\n","                X, Y : List of association words\n","                A, B : List of target words\n","                embd : Dictonary of word-to-embedding for all words\n","                sample : Number of random permutations used.\n","        Returns\n","    \"\"\"\n","    size_of_permutation = min(len(X), len(Y))\n","    X_Y = X + Y\n","    test_stats_over_permutation = []\n","    \n","    Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n","    Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n","    Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n","    Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n","    \n","    if not sample:\n","        permutations = combinations(X_Y, size_of_permutation)\n","    else:\n","        permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n","        \n","    for Xi in permutations:\n","        Yi = filterfalse(lambda w:w in Xi, X_Y)\n","        Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n","        Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n","        test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n","        \n","    unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n","    \n","    is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n","    \n","    return is_over.sum() / is_over.size"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pdo3YyjgKXzs","colab_type":"text"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"OuvUghoSl5tF","colab_type":"code","colab":{}},"source":["def category(type=1):\n","    m_words = \"male, man, boy, brother, he, him, his, son, father, uncle, grandfather\".lower().replace(\" \", \"\").split(',')\n","    f_words = \"female, woman, girl, sister, she, her, hers, daughter, mother, aunt, grandmother\".lower().replace(\" \", \"\").split(',')\n","    if type == 1:\n","        #B1: career vs family\n","        x_words = \"executive, management, professional, corporation, salary, office, business, career\".lower().replace(\" \", \"\").split(',')\n","        y_words = \"home, parents, children, family, cousins, marriage, wedding, relatives\".lower().replace(\" \", \"\").split(',')\n","    elif type == 2:\n","        #B2: maths vs arts\n","        x_words = \"math, algebra, geometry, calculus, equations, computation, numbers, addition\".lower().replace(\" \", \"\").split(',')\n","        y_words = \"poetry, art, Shakespeare, dance, literature, novel, symphony, drama\".lower().replace(\" \", \"\").split(',')\n","    elif type == 3:\n","        #B3: science vs arts\n","        x_words = \"science, technology, physics, chemistry, Einstein, NASA, experiment, astronomy\".lower().replace(\" \", \"\").split(',')\n","        y_words = \"poetry, art, Shakespeare, dance, literature, novel, symphony, drama\".lower().replace(\" \", \"\").split(',')\n","    elif type == 4:\n","        #B4: intelligence vs appearance\n","        x_words = \"precocious, resourceful, inquisitive, genius, inventive, astute, adaptable, reflective,discerning, intuitive, inquiring, judicious, analytical, apt, venerable, imaginative,shrewd, thoughtful, wise, smart, ingenious, clever, brilliant, logical, intelligent\".lower().replace(\" \", \"\").split(',')\n","        y_words = \"alluring, voluptuous, blushing, homely, plump, sensual, gorgeous, slim, bald,athletic, fashionable, stout, ugly, muscular, slender, feeble, handsome, healthy,attractive, fat, weak, thin, pretty, beautiful, strong\".lower().replace(\" \", \"\").split(',')\n","    elif type == 5:\n","        #B5: strength vs weakness\n","        x_words = \"power, strong, confident, dominant, potent, command, assert, loud, bold, succeed,triumph, leader, shout, dynamic, winner\".lower().replace(\" \", \"\").split(',')\n","        y_words = \"weak, surrender, timid, vulnerable, weakness, wispy, withdraw, yield, failure, shy,follow, lose, fragile, afraid, loser\".lower().replace(\" \", \"\").split(',')\n","    return m_words, f_words, x_words, y_words\n","\n","def category_2(type=1):\n","    if type == 1:\n","        #Career/Family\n","        m_words = 'John, Paul, Mike, Kevin, Steve, Greg, Jeff, Bill'.lower().replace(\" \", \"\").split(',')\n","        f_words = 'Amy, Joan, Lisa, Sarah, Diana, Kate, Ann, Donna'.lower().replace(\" \", \"\").split(',')\n","        x_words = 'executive, management, professional, corporation, salary, office, business, career'.lower().replace(\" \", \"\").split(',')\n","        y_words = 'home, parents, children, family, cousins, marriage, wedding, relatives'.lower().replace(\" \", \"\").split(',')\n","    elif type == 2:\n","        #Math/Art\n","        m_words = 'math, algebra, geometry, calculus, equations, computation, numbers, addition'.lower().replace(\" \", \"\").split(',')\n","        f_words = 'poetry, art, dance, literature, novel, symphony, drama, sculpture'.lower().replace(\" \", \"\").split(',')\n","        x_words = 'male, man, boy, brother, he, him, his, son'.lower().replace(\" \", \"\").split(',')\n","        y_words = 'female, woman, girl, sister, she, her, hers, daughter'.lower().replace(\" \", \"\").split(',')\n","    elif type == 3:\n","        #Science/Art\n","        m_words = 'science, technology, physics, chemistry, Einstein, NASA, experiment, astronomy'.lower().replace(\" \", \"\").split(',')\n","        f_words = 'poetry, art, Shakespeare, dance, literature, novel, symphony, drama'.lower().replace(\" \", \"\").split(',')\n","        x_words = 'brother, father, uncle, grandfather, son, he, his, him'.lower().replace(\" \", \"\").split(',')\n","        y_words = 'sister, mother, aunt, grandmother, daughter, she, hers, her'.lower().replace(\" \", \"\").split(',')\n","    return m_words, f_words, x_words, y_words"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SvwnbWHdxCJs","colab_type":"text"},"source":["# Get Embeddings"]},{"cell_type":"code","metadata":{"id":"gFn83YmOpHUY","colab_type":"code","colab":{}},"source":["import densray_bert\n","\n","\n","def get_eigvecs_dict(layer=-1):\n","    eigvecs_dict = {}\n","    #-1:apply to all layers\n","    if layer == -1:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = ('/content/drive/My Drive/eigvecs_'+config+'_noavg_'+str(nsamples)+'_'+str(l)+'.pt', True)\n","    elif layer ==-2:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = ('/content/drive/My Drive/eigvecs_'+config+'_noavg_'+str(nsamples)+'_'+str(l)+'.pt', False)\n","    else:\n","        for l in range(nlayer):\n","            if l==layer:\n","                eigvecs_dict[str(l)] = ('/content/drive/My Drive/eigvecs_'+config+'_noavg_'+str(nsamples)+'_'+str(l)+'.pt', True)\n","            else:\n","                eigvecs_dict[str(l)] = ('/content/drive/My Drive/eigvecs_'+config+'_noavg_'+str(nsamples)+'_'+str(l)+'.pt', False)\n","    return eigvecs_dict\n","\n","\n","def get_bert_embedding(model, wordlist, is_targets=1):\n","    vecss = torch.Tensor().to(device)\n","    for w in wordlist:\n","        text = w + ' is ' + tokenizer.mask_token + '.' if is_targets else tokenizer.mask_token + ' is ' + w + '.'\n","        vec = tokenizer.prepare_for_model(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)),\n","                                            return_token_type_ids=False, return_tensors='pt')['input_ids'].to(device)\n","        vecs = vec.clone().detach()\n","        # get output\n","        vecs = model.bert(vecs)[0]#[2][nlayer]\n","        vecs = vecs[0][1:-4,:].mean(dim=0).unsqueeze(0) if is_targets else vecs[0][3:-2,:].mean(dim=0).unsqueeze(0)\n","        vecss = torch.cat((vecss,vecs))\n","    return vecss\n","\n","def eval_per_layer(layer=-2):\n","    config_class = get_eigvecs_dict(layer)\n","    model = densray_bert.BertForMaskedLM_1.from_pretrained('bert-'+config+'-uncased', eigvecs_dict=get_eigvecs_dict(l)).to(device)\n","    # turn on eval mode\n","    model.eval()\n","    m = get_bert_embedding(model, m_words, is_targets=0).cpu().detach().numpy()\n","    f = get_bert_embedding(model, f_words, is_targets=0).cpu().detach().numpy()\n","    x = get_bert_embedding(model, x_words, is_targets=1).cpu().detach().numpy()\n","    y = get_bert_embedding(model, y_words, is_targets=1).cpu().detach().numpy()\n","    embed = {}\n","    for i in range(len(m_words)): embed[m_words[i]] = m[i]\n","    for i in range(len(f_words)): embed[f_words[i]] = f[i]\n","    for i in range(len(x_words)): embed[x_words[i]] = x[i]\n","    for i in range(len(y_words)): embed[y_words[i]] = y[i]\n","    return embed"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IHkY40G5KpbY","colab_type":"text"},"source":["# Go!"]},{"cell_type":"code","metadata":{"id":"2nTgkJtD814V","colab_type":"code","outputId":"a65f4221-4bea-4898-e55d-f67c6b2840a2","executionInfo":{"status":"ok","timestamp":1588248555644,"user_tz":-120,"elapsed":1760952,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for t in range(1,4):\n","    m_words, f_words, x_words, y_words = category_2(t)\n","    print('d    d_densray |d|-|d_densray|   p   p-densray   p_densray-p')\n","    l=-2\n","    # no densray\n","    embed = eval_per_layer(layer=l)\n","    d =  weat_effect_size(x_words, y_words, m_words, f_words, embed)\n","    p = weat_p_value(x_words, y_words, m_words, f_words, embed, sample=1000)\n","    #densray\n","    for l in range(-1, nlayer):\n","        # densray\n","        embed = eval_per_layer(layer=l)\n","        d_densray =  weat_effect_size(x_words, y_words, m_words, f_words, embed)\n","        p_densray = weat_p_value(x_words, y_words, m_words, f_words, embed, sample=1000)\n","        print(round(d,4), round(d_densray,4), round(abs(d)-abs(d_densray),4), \n","              round(p,4), round(p_densray,4), round(p_densray-p,4))\n","    print('\\n')"],"execution_count":29,"outputs":[{"output_type":"stream","text":["d    d_densray |d|-|d_densray|   p   p-densray   p_densray-p\n","1.5705 0.8143 0.7562 0.0 0.053 0.053\n","1.5705 1.5282 0.0423 0.0 0.0 0.0\n","1.5705 1.5007 0.0698 0.0 0.0 0.0\n","1.5705 1.5095 0.0609 0.0 0.0 0.0\n","1.5705 1.3763 0.1942 0.0 0.001 0.001\n","1.5705 1.4469 0.1236 0.0 0.0 0.0\n","1.5705 1.5604 0.01 0.0 0.0 0.0\n","1.5705 1.4713 0.0992 0.0 0.0 0.0\n","1.5705 1.4277 0.1428 0.0 0.0 0.0\n","1.5705 1.4517 0.1188 0.0 0.0 0.0\n","1.5705 1.4118 0.1587 0.0 0.0 0.0\n","1.5705 1.5418 0.0287 0.0 0.0 0.0\n","1.5705 1.5268 0.0437 0.0 0.0 0.0\n","1.5705 1.5443 0.0262 0.0 0.0 0.0\n","1.5705 1.4957 0.0748 0.0 0.0 0.0\n","1.5705 1.1701 0.4003 0.0 0.009 0.009\n","1.5705 1.0482 0.5222 0.0 0.018 0.018\n","1.5705 0.9637 0.6068 0.0 0.026 0.026\n","1.5705 0.9731 0.5974 0.0 0.024 0.024\n","1.5705 1.0097 0.5608 0.0 0.021 0.021\n","1.5705 1.0048 0.5657 0.0 0.021 0.021\n","1.5705 1.0219 0.5485 0.0 0.023 0.023\n","1.5705 0.9975 0.5729 0.0 0.027 0.027\n","1.5705 1.0536 0.5169 0.0 0.019 0.019\n","1.5705 1.0563 0.5142 0.0 0.019 0.019\n","\n","\n","d    d_densray |d|-|d_densray|   p   p-densray   p_densray-p\n","-0.4009 0.0413 0.3595 0.753 0.455 -0.298\n","-0.4009 -0.1108 0.29 0.753 0.572 -0.181\n","-0.4009 -0.0797 0.3212 0.753 0.548 -0.205\n","-0.4009 -0.0906 0.3102 0.753 0.559 -0.194\n","-0.4009 -0.1926 0.2083 0.753 0.628 -0.125\n","-0.4009 -0.1871 0.2138 0.753 0.614 -0.139\n","-0.4009 -0.2128 0.1881 0.753 0.636 -0.117\n","-0.4009 -0.2253 0.1756 0.753 0.64 -0.113\n","-0.4009 -0.2533 0.1475 0.753 0.657 -0.096\n","-0.4009 -0.2348 0.1661 0.753 0.65 -0.103\n","-0.4009 -0.1715 0.2293 0.753 0.607 -0.146\n","-0.4009 -0.1072 0.2936 0.753 0.562 -0.191\n","-0.4009 -0.0877 0.3132 0.753 0.546 -0.207\n","-0.4009 -0.2041 0.1967 0.753 0.627 -0.126\n","-0.4009 -0.2983 0.1025 0.753 0.683 -0.07\n","-0.4009 -0.2707 0.1302 0.753 0.66 -0.093\n","-0.4009 -0.2227 0.1781 0.753 0.636 -0.117\n","-0.4009 -0.1947 0.2061 0.753 0.62 -0.133\n","-0.4009 -0.2154 0.1855 0.753 0.626 -0.127\n","-0.4009 -0.4122 -0.0113 0.753 0.754 0.001\n","-0.4009 -0.4322 -0.0313 0.753 0.766 0.013\n","-0.4009 -0.5216 -0.1207 0.753 0.822 0.069\n","-0.4009 -0.3878 0.0131 0.753 0.739 -0.014\n","-0.4009 -0.4332 -0.0324 0.753 0.774 0.021\n","-0.4009 -0.393 0.0079 0.753 0.747 -0.006\n","\n","\n","d    d_densray |d|-|d_densray|   p   p-densray   p_densray-p\n","-0.596 0.1872 0.4088 0.873 0.338 -0.535\n","-0.596 -0.1761 0.4199 0.873 0.631 -0.242\n","-0.596 -0.1645 0.4315 0.873 0.629 -0.244\n","-0.596 -0.1581 0.438 0.873 0.623 -0.25\n","-0.596 -0.3887 0.2073 0.873 0.767 -0.106\n","-0.596 -0.3524 0.2436 0.873 0.754 -0.119\n","-0.596 -0.2845 0.3116 0.873 0.705 -0.168\n","-0.596 -0.3123 0.2837 0.873 0.725 -0.148\n","-0.596 -0.3074 0.2886 0.873 0.723 -0.15\n","-0.596 -0.236 0.36 0.873 0.675 -0.198\n","-0.596 -0.1815 0.4145 0.873 0.637 -0.236\n","-0.596 -0.1404 0.4556 0.873 0.613 -0.26\n","-0.596 -0.0798 0.5163 0.873 0.56 -0.313\n","-0.596 -0.1094 0.4866 0.873 0.577 -0.296\n","-0.596 -0.1815 0.4146 0.873 0.636 -0.237\n","-0.596 -0.1518 0.4442 0.873 0.621 -0.252\n","-0.596 -0.2061 0.3899 0.873 0.649 -0.224\n","-0.596 -0.1991 0.3969 0.873 0.644 -0.229\n","-0.596 -0.2242 0.3718 0.873 0.661 -0.212\n","-0.596 -0.3565 0.2395 0.873 0.75 -0.123\n","-0.596 -0.4948 0.1012 0.873 0.835 -0.038\n","-0.596 -0.6384 -0.0424 0.873 0.887 0.014\n","-0.596 -0.5158 0.0802 0.873 0.838 -0.035\n","-0.596 -0.5323 0.0637 0.873 0.84 -0.033\n","-0.596 -0.4659 0.1301 0.873 0.814 -0.059\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PpvAEp76A3R8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wsx5wTbfA3aD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LihkhEoaA3dP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lt2I_oTtA3iz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2rzlSkklA3l0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJZw5G9OA3gn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"b87e95e2-a73a-4485-dbee-d1f41ce990d0","executionInfo":{"status":"ok","timestamp":1587444534722,"user_tz":-120,"elapsed":713635,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"id":"Y0F4rc1L42nN","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":[""],"execution_count":0,"outputs":[{"output_type":"stream","text":["d    d_densray |d|-|d_densray|   p   p-densray   p_densray-p\n","1.0083 0.1539 0.8544 0.02 0.353 0.333\n","1.0083 0.7002 0.3082 0.02 0.089 0.069\n","1.0083 0.6382 0.3702 0.02 0.116 0.096\n","1.0083 0.6179 0.3904 0.02 0.122 0.102\n","1.0083 0.8179 0.1904 0.02 0.056 0.036\n","1.0083 0.792 0.2163 0.02 0.065 0.045\n","1.0083 1.06 -0.0517 0.02 0.019 -0.001\n","1.0083 0.9506 0.0577 0.02 0.013 -0.007\n","1.0083 0.9207 0.0877 0.02 0.015 -0.005\n","1.0083 1.2032 -0.1949 0.02 0.002 -0.018\n","1.0083 1.0765 -0.0681 0.02 0.003 -0.017\n","1.0083 1.2116 -0.2033 0.02 0.006 -0.014\n","1.0083 0.9429 0.0654 0.02 0.034 0.014\n","1.0083 0.9473 0.061 0.02 0.033 0.013\n","1.0083 1.1504 -0.142 0.02 0.004 -0.016\n","1.0083 1.0471 -0.0387 0.02 0.0 -0.02\n","1.0083 0.9258 0.0825 0.02 0.001 -0.019\n","1.0083 0.9291 0.0793 0.02 0.001 -0.019\n","1.0083 1.0881 -0.0798 0.02 0.002 -0.018\n","1.0083 1.1217 -0.1134 0.02 0.001 -0.019\n","1.0083 1.0004 0.0079 0.02 0.005 -0.015\n","1.0083 0.9316 0.0767 0.02 0.007 -0.013\n","1.0083 0.7355 0.2729 0.02 0.074 0.054\n","1.0083 0.7182 0.2901 0.02 0.077 0.057\n","1.0083 0.788 0.2203 0.02 0.056 0.036\n","\n","\n","d    d_densray |d|-|d_densray|   p   p-densray   p_densray-p\n","0.4899 0.3239 0.166 0.183 0.248 0.065\n","0.4899 0.7598 -0.2699 0.183 0.065 -0.118\n","0.4899 0.7943 -0.3044 0.183 0.061 -0.122\n","0.4899 0.7238 -0.2339 0.183 0.08 -0.103\n","0.4899 0.5235 -0.0336 0.183 0.165 -0.018\n","0.4899 0.5342 -0.0443 0.183 0.155 -0.028\n","0.4899 0.6956 -0.2056 0.183 0.092 -0.091\n","0.4899 0.2839 0.206 0.183 0.302 0.119\n","0.4899 0.5074 -0.0175 0.183 0.175 -0.008\n","0.4899 -0.4442 0.0457 0.183 0.793 0.61\n","0.4899 -0.015 0.4749 0.183 0.526 0.343\n","0.4899 0.7141 -0.2242 0.183 0.079 -0.104\n","0.4899 0.6952 -0.2053 0.183 0.084 -0.099\n","0.4899 0.6047 -0.1148 0.183 0.129 -0.054\n","0.4899 0.6376 -0.1477 0.183 0.114 -0.069\n","0.4899 1.0467 -0.5567 0.183 0.023 -0.16\n","0.4899 0.4855 0.0044 0.183 0.199 0.016\n","0.4899 0.4487 0.0412 0.183 0.205 0.022\n","0.4899 0.8227 -0.3328 0.183 0.063 -0.12\n","0.4899 0.8276 -0.3376 0.183 0.064 -0.119\n","0.4899 0.9242 -0.4343 0.183 0.04 -0.143\n","0.4899 0.7487 -0.2588 0.183 0.071 -0.112\n","0.4899 0.9205 -0.4306 0.183 0.031 -0.152\n","0.4899 0.5039 -0.014 0.183 0.159 -0.024\n","0.4899 0.3432 0.1468 0.183 0.242 0.059\n","\n","\n","d    d_densray |d|-|d_densray|   p   p-densray   p_densray-p\n","0.85 0.4432 0.4069 0.029 0.216 0.187\n","0.85 0.8961 -0.0461 0.029 0.01 -0.019\n","0.85 1.0138 -0.1638 0.029 0.011 -0.018\n","0.85 0.8749 -0.0248 0.029 0.015 -0.014\n","0.85 0.866 -0.016 0.029 0.044 0.015\n","0.85 0.9212 -0.0712 0.029 0.035 0.006\n","0.85 1.0116 -0.1616 0.029 0.009 -0.02\n","0.85 1.1498 -0.2998 0.029 0.011 -0.018\n","0.85 1.0894 -0.2394 0.029 0.008 -0.021\n","0.85 1.0926 -0.2426 0.029 0.016 -0.013\n","0.85 1.0336 -0.1836 0.029 0.019 -0.01\n","0.85 0.8894 -0.0394 0.029 0.0 -0.029\n","0.85 0.7945 0.0556 0.029 0.006 -0.023\n","0.85 0.7501 0.0999 0.029 0.018 -0.011\n","0.85 0.8247 0.0254 0.029 0.013 -0.016\n","0.85 0.908 -0.0579 0.029 0.002 -0.027\n","0.85 0.7748 0.0753 0.029 0.015 -0.014\n","0.85 0.7625 0.0876 0.029 0.014 -0.015\n","0.85 1.0091 -0.159 0.029 0.001 -0.028\n","0.85 0.9757 -0.1257 0.029 0.002 -0.027\n","0.85 0.9841 -0.1341 0.029 0.002 -0.027\n","0.85 0.9169 -0.0668 0.029 0.001 -0.028\n","0.85 0.9056 -0.0555 0.029 0.003 -0.026\n","0.85 0.7674 0.0826 0.029 0.024 -0.005\n","0.85 0.7926 0.0574 0.029 0.018 -0.011\n","\n","\n","d    d_densray |d|-|d_densray|   p   p-densray   p_densray-p\n","-0.1678 0.0679 0.0999 0.719 0.398 -0.321\n","-0.1678 -0.4177 -0.25 0.719 0.936 0.217\n","-0.1678 -0.4516 -0.2838 0.719 0.945 0.226\n","-0.1678 -0.4275 -0.2597 0.719 0.941 0.222\n","-0.1678 -0.3372 -0.1694 0.719 0.884 0.165\n","-0.1678 -0.3835 -0.2157 0.719 0.912 0.193\n","-0.1678 -0.2462 -0.0784 0.719 0.804 0.085\n","-0.1678 -0.2097 -0.0419 0.719 0.762 0.043\n","-0.1678 -0.3334 -0.1656 0.719 0.885 0.166\n","-0.1678 -0.1157 0.0521 0.719 0.642 -0.077\n","-0.1678 -0.1594 0.0084 0.719 0.706 -0.013\n","-0.1678 -0.1682 -0.0005 0.719 0.709 -0.01\n","-0.1678 -0.3774 -0.2096 0.719 0.912 0.193\n","-0.1678 -0.3325 -0.1647 0.719 0.875 0.156\n","-0.1678 -0.1788 -0.011 0.719 0.739 0.02\n","-0.1678 0.1135 0.0543 0.719 0.357 -0.362\n","-0.1678 -0.1125 0.0553 0.719 0.658 -0.061\n","-0.1678 0.1806 -0.0128 0.719 0.269 -0.45\n","-0.1678 0.1487 0.0191 0.719 0.302 -0.417\n","-0.1678 0.3236 -0.1559 0.719 0.146 -0.573\n","-0.1678 0.1603 0.0075 0.719 0.289 -0.43\n","-0.1678 0.1951 -0.0273 0.719 0.254 -0.465\n","-0.1678 0.0362 0.1316 0.719 0.456 -0.263\n","-0.1678 -0.1108 0.057 0.719 0.652 -0.067\n","-0.1678 -0.0103 0.1575 0.719 0.519 -0.2\n","\n","\n","d    d_densray |d|-|d_densray|   p   p-densray   p_densray-p\n","0.5909 0.3488 0.2421 0.046 0.16 0.114\n","0.5909 0.4096 0.1814 0.046 0.141 0.095\n","0.5909 0.5793 0.0117 0.046 0.049 0.003\n","0.5909 0.4471 0.1439 0.046 0.114 0.068\n","0.5909 0.663 -0.0721 0.046 0.032 -0.014\n","0.5909 0.6107 -0.0197 0.046 0.045 -0.001\n","0.5909 0.4328 0.1582 0.046 0.113 0.067\n","0.5909 0.6144 -0.0235 0.046 0.041 -0.005\n","0.5909 0.5341 0.0568 0.046 0.074 0.028\n","0.5909 0.671 -0.08 0.046 0.026 -0.02\n","0.5909 0.6795 -0.0886 0.046 0.026 -0.02\n","0.5909 0.4526 0.1384 0.046 0.109 0.063\n","0.5909 0.5955 -0.0046 0.046 0.053 0.007\n","0.5909 0.5935 -0.0025 0.046 0.058 0.012\n","0.5909 0.5484 0.0426 0.046 0.073 0.027\n","0.5909 0.1399 0.451 0.046 0.371 0.325\n","0.5909 0.3222 0.2687 0.046 0.228 0.182\n","0.5909 0.0093 0.5816 0.046 0.488 0.442\n","0.5909 0.0355 0.5554 0.046 0.462 0.416\n","0.5909 0.0924 0.4985 0.046 0.404 0.358\n","0.5909 0.0434 0.5475 0.046 0.457 0.411\n","0.5909 -0.0167 0.5742 0.046 0.512 0.466\n","0.5909 -0.0128 0.5781 0.046 0.515 0.469\n","0.5909 0.2302 0.3607 0.046 0.271 0.225\n","0.5909 0.4454 0.1455 0.046 0.109 0.063\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4_BbF4ckFEVg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}