{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of SEAT&CEAT.ipynb","provenance":[{"file_id":"1XKR70fSepPBzLMXXSWpxh_0RzSutjoun","timestamp":1593018679815},{"file_id":"1kucQWqQDONbcXtYGg5Y0L_AI90pl2u6K","timestamp":1592969883125},{"file_id":"1QsCRMwPNffdfgjR_jvW3O83h2uyiGxi4","timestamp":1589947460129},{"file_id":"14320o-9-uc0nsejqTeNe66uhFYw4QSU9","timestamp":1589939467638},{"file_id":"1ePa9C1bq3wS5WByfi8L9hCBGqtl2HhWu","timestamp":1588134391908},{"file_id":"1c__r6mWy4-vOfdfL4tmqij72Qayz2yHY","timestamp":1586403317137},{"file_id":"1OCwvXO9cB1Ke5Bi11SvjovgCKDh6iE6M","timestamp":1586403087992},{"file_id":"1tgzcbuJrxhBeC6FnQig4Z8RLj3TD2Y-p","timestamp":1586402323174},{"file_id":"1gnY3Blunw8HAUxw8wAFCzirazvafbmQn","timestamp":1586399576485},{"file_id":"1GHdeIzOlLZLZ2RN2MohSwBfpWuNi1ogN","timestamp":1583364612376},{"file_id":"1BA0JTleU0JVbJjgrXw5ULWcTa5Nl5sGb","timestamp":1583315004957},{"file_id":"1xWrKImFLCBga34NpEp9EoaMc2AfUPXIK","timestamp":1583286864919}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a1f0e5b58d684b04a7da4b5a7a23bed1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2907cfae50064c26bd84b777824b3d3b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_627cd8d9cf3a447286acf3923244256d","IPY_MODEL_67c0e1e762c44198b6ef7b535e80507b"]}},"2907cfae50064c26bd84b777824b3d3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"627cd8d9cf3a447286acf3923244256d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_600bd25b22a5453dbfdbaf22faec35f9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":434,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":434,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_52758ecdab1645e7aca993b860236588"}},"67c0e1e762c44198b6ef7b535e80507b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1dab343ee7604541bda1e2cc1831cfc8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 434/434 [00:38&lt;00:00, 11.3B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f136c826ea043a09ae58052cc3f6904"}},"600bd25b22a5453dbfdbaf22faec35f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"52758ecdab1645e7aca993b860236588":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1dab343ee7604541bda1e2cc1831cfc8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4f136c826ea043a09ae58052cc3f6904":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"77458cecda9c4a1f9b6b873ded06bfd4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_34f903277e9c48199d43a32cd8b0f690","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7ec91a3b557b4da8ab33fc8adb59843a","IPY_MODEL_318360708a9f4f56926d1c0c1a1fffdc"]}},"34f903277e9c48199d43a32cd8b0f690":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ec91a3b557b4da8ab33fc8adb59843a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_95af766b976c43e695200d0206067958","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1344997306,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1344997306,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_18db9da84a6441888b76043ad856d2ba"}},"318360708a9f4f56926d1c0c1a1fffdc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cb73608d409344d0b0044e538f5c2696","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.34G/1.34G [00:37&lt;00:00, 35.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cfb9712ee9dd416683e66cf2e2875eda"}},"95af766b976c43e695200d0206067958":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"18db9da84a6441888b76043ad856d2ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb73608d409344d0b0044e538f5c2696":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cfb9712ee9dd416683e66cf2e2875eda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1f9c3169abfe4dd09cd0e69cc20297d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e5b8eb23ef9f425b8600ce0f86d11474","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dd3c52055c4a4f8082a65141fb1e4c10","IPY_MODEL_98ed558436174fc1a7114e853e9e790b"]}},"e5b8eb23ef9f425b8600ce0f86d11474":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dd3c52055c4a4f8082a65141fb1e4c10":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7753a0e8a69146f18d0fe546b6722d1f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_01ee320c08a3484b86ab75ebec87c981"}},"98ed558436174fc1a7114e853e9e790b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f0b9873cde1b42d9a691f20a47cfb822","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 2.92MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_182bf472af634f32b8c433bcf4bfc6d1"}},"7753a0e8a69146f18d0fe546b6722d1f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"01ee320c08a3484b86ab75ebec87c981":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f0b9873cde1b42d9a691f20a47cfb822":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"182bf472af634f32b8c433bcf4bfc6d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"q2O3yi9gUjY-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":665},"executionInfo":{"status":"ok","timestamp":1593584308771,"user_tz":-120,"elapsed":8916,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"b8bfd465-a7ee-4f5b-8eb0-1b28da3a1889"},"source":["!pip install transformers==2.8.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers==2.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n","\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.5)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 13.7MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 31.7MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 46.2MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.41.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.14.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.6.20)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.15.1)\n","Requirement already satisfied: botocore<1.18.0,>=1.17.9 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.17.9)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.10.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->transformers==2.8.0) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->transformers==2.8.0) (0.15.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=349404a23dc2acdb0e5be73ade5a91cab62ab279d8e661048f108d936e3d5694\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.5.2 transformers-2.8.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5rMiCKAusFtK","colab_type":"text"},"source":["# Config"]},{"cell_type":"code","metadata":{"id":"Jzx0-7HbaPf6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1593584335065,"user_tz":-120,"elapsed":33756,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"635a1164-cc6f-4ddf-d276-c208e81079ca"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lyUbLYhu3m4b","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a1f0e5b58d684b04a7da4b5a7a23bed1","2907cfae50064c26bd84b777824b3d3b","627cd8d9cf3a447286acf3923244256d","67c0e1e762c44198b6ef7b535e80507b","600bd25b22a5453dbfdbaf22faec35f9","52758ecdab1645e7aca993b860236588","1dab343ee7604541bda1e2cc1831cfc8","4f136c826ea043a09ae58052cc3f6904","77458cecda9c4a1f9b6b873ded06bfd4","34f903277e9c48199d43a32cd8b0f690","7ec91a3b557b4da8ab33fc8adb59843a","318360708a9f4f56926d1c0c1a1fffdc","95af766b976c43e695200d0206067958","18db9da84a6441888b76043ad856d2ba","cb73608d409344d0b0044e538f5c2696","cfb9712ee9dd416683e66cf2e2875eda","1f9c3169abfe4dd09cd0e69cc20297d8","e5b8eb23ef9f425b8600ce0f86d11474","dd3c52055c4a4f8082a65141fb1e4c10","98ed558436174fc1a7114e853e9e790b","7753a0e8a69146f18d0fe546b6722d1f","01ee320c08a3484b86ab75ebec87c981","f0b9873cde1b42d9a691f20a47cfb822","182bf472af634f32b8c433bcf4bfc6d1"]},"executionInfo":{"status":"ok","timestamp":1593584404669,"user_tz":-120,"elapsed":58448,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"d66a6fc8-be65-493d-e0cf-14ffb1c4385f"},"source":["import torch\n","import transformers\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","config = 'large'\n","nlayer = 12 if config == 'base' else 24\n","nsamples = 5000 if config == 'base' else 10000\n","\n","model = transformers.BertForMaskedLM.from_pretrained('bert-'+config+'-uncased', output_hidden_states=True).to(device)\n","tokenizer = transformers.BertTokenizer.from_pretrained('bert-'+config+'-uncased')\n","# turn on eval mode\n","model.eval()"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1f0e5b58d684b04a7da4b5a7a23bed1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=434.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"77458cecda9c4a1f9b6b873ded06bfd4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1344997306.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f9c3169abfe4dd09cd0e69cc20297d8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["BertForMaskedLM(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n","      (position_embeddings): Embedding(512, 1024)\n","      (token_type_embeddings): Embedding(2, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (cls): BertOnlyMLMHead(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (decoder): Linear(in_features=1024, out_features=30522, bias=True)\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"eAEafXm9L8p2","colab_type":"text"},"source":["# WEAT"]},{"cell_type":"code","metadata":{"id":"mf6_liysF8en","colab_type":"code","colab":{}},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","def swAB(W, A, B):\n","    \"\"\"Calculates differential cosine-similarity between word vectors in W, A and W, B\n","        Arguments\n","                W, A, B : n x d matrix of word embeddings stored row wise\n","    \"\"\"\n","    WA = cosine_similarity(W,A)\n","    WB = cosine_similarity(W,B)\n","    \n","    #Take mean along columns\n","    WAmean = np.mean(WA, axis = 1)\n","    WBmean = np.mean(WB, axis = 1)\n","    \n","    return (WAmean - WBmean)\n","  \n","def test_statistic(X, Y, A, B):\n","    \"\"\"Calculates test-statistic between the pair of association words and target words\n","        Arguments\n","                X, Y, A, B : n x d matrix of word embeddings stored row wise\n","        Returns\n","                Test Statistic\n","    \"\"\"\n","    return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))\n","\n","def weat_effect_size(X, Y, A, B, embd):\n","    \"\"\"Computes the effect size for the given list of association and target word pairs\n","        Arguments\n","                X, Y : List of association words\n","                A, B : List of target words\n","                embd : Dictonary of word-to-embedding for all words\n","        Returns\n","                Effect Size\n","    \"\"\"\n","    Xmat = np.array([embd[w] for w in X if w in embd])\n","    Ymat = np.array([embd[w] for w in Y if w in embd])\n","    Amat = np.array([embd[w] for w in A if w in embd])\n","    Bmat = np.array([embd[w] for w in B if w in embd])\n","    XuY = list(set(X).union(Y))\n","    XuYmat = []\n","    for w in XuY:\n","        if w.lower() in embd:\n","            XuYmat.append(embd[w.lower()])\n","    XuYmat = np.array(XuYmat)\n","    d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n","    return d\n","\n","def random_permutation(iterable, r=None):\n","    \"\"\"Returns a random permutation for any iterable object\"\"\"\n","    pool = tuple(iterable)\n","    r = len(pool) if r is None else r\n","    return tuple(random.sample(pool, r))\n","\n","def weat_p_value(X, Y, A, B, embd, sample=None):\n","    np.random.seed(42)\n","    random.seed(42)\n","    \"\"\"Computes the one-sided P value for the given list of association and target word pairs\n","        Arguments\n","                X, Y : List of association words\n","                A, B : List of target words\n","                embd : Dictonary of word-to-embedding for all words\n","                sample : Number of random permutations used.\n","        Returns\n","    \"\"\"\n","    size_of_permutation = min(len(X), len(Y))\n","    X_Y = X + Y\n","    test_stats_over_permutation = []\n","    \n","    Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n","    Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n","    Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n","    Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n","    \n","    if not sample:\n","        permutations = combinations(X_Y, size_of_permutation)\n","    else:\n","        permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n","        \n","    for Xi in permutations:\n","        Yi = filterfalse(lambda w:w in Xi, X_Y)\n","        Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n","        Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n","        test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n","        \n","    unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n","    \n","    is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n","    \n","    return is_over.sum() / is_over.size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQwlTrP8L-fN","colab_type":"code","colab":{}},"source":["def category(type=1):\n","    m_words = \"male, man, boy, brother, he, him, his, son, father, uncle, grandfather\".lower().replace(\" \", \"\").split(',')\n","    f_words = \"female, woman, girl, sister, she, her, hers, daughter, mother, aunt, grandmother\".lower().replace(\" \", \"\").split(',')\n","    if type == 1:\n","        #B1: career vs family\n","        x_words = \"executive, management, professional, corporation, salary, office, business, career\".lower().replace(\" \", \"\").split(',')\n","        y_words = \"home, parents, children, family, cousins, marriage, wedding, relatives\".lower().replace(\" \", \"\").split(',')\n","    elif type == 2:\n","        #B2: maths vs arts\n","        x_words = \"math, algebra, geometry, calculus, equations, computation, numbers, addition\".lower().replace(\" \", \"\").split(',')\n","        y_words = \"poetry, art, Shakespeare, dance, literature, novel, symphony, drama\".lower().replace(\" \", \"\").split(',')\n","    elif type == 3:\n","        #B3: science vs arts\n","        x_words = \"science, technology, physics, chemistry, Einstein, NASA, experiment, astronomy\".lower().replace(\" \", \"\").split(',')\n","        y_words = \"poetry, art, Shakespeare, dance, literature, novel, symphony, drama\".lower().replace(\" \", \"\").split(',')\n","    elif type == 4:\n","        #B4: intelligence vs appearance\n","        x_words = \"precocious, resourceful, inquisitive, genius, inventive, astute, adaptable, reflective,discerning, intuitive, inquiring, judicious, analytical, apt, venerable, imaginative,shrewd, thoughtful, wise, smart, ingenious, clever, brilliant, logical, intelligent\".lower().replace(\" \", \"\").split(',')\n","        y_words = \"alluring, voluptuous, blushing, homely, plump, sensual, gorgeous, slim, bald,athletic, fashionable, stout, ugly, muscular, slender, feeble, handsome, healthy,attractive, fat, weak, thin, pretty, beautiful, strong\".lower().replace(\" \", \"\").split(',')\n","    elif type == 5:\n","        #B5: strength vs weakness\n","        x_words = \"power, strong, confident, dominant, potent, command, assert, loud, bold, succeed,triumph, leader, shout, dynamic, winner\".lower().replace(\" \", \"\").split(',')\n","        y_words = \"weak, surrender, timid, vulnerable, weakness, wispy, withdraw, yield, failure, shy,follow, lose, fragile, afraid, loser\".lower().replace(\" \", \"\").split(',')\n","    return m_words, f_words, x_words, y_words\n","\n","def category_2(type=1):\n","    if type == 1:\n","        #Career/Family\n","        x_words = 'John, Paul, Mike, Kevin, Steve, Greg, Jeff, Bill'.lower().replace(\" \", \"\").split(',')\n","        y_words = 'Amy, Joan, Lisa, Sarah, Diana, Kate, Ann, Donna'.lower().replace(\" \", \"\").split(',')\n","        m_words = 'executive, management, professional, corporation, salary, office, business, career'.lower().replace(\" \", \"\").split(',')\n","        f_words = 'home, parents, children, family, cousins, marriage, wedding, relatives'.lower().replace(\" \", \"\").split(',')\n","    elif type == 2:\n","        #Math/Art\n","        x_words = 'math, algebra, geometry, calculus, equations, computation, numbers, addition'.lower().replace(\" \", \"\").split(',')\n","        y_words = 'poetry, art, dance, literature, novel, symphony, drama, sculpture'.lower().replace(\" \", \"\").split(',')\n","        m_words = 'male, man, boy, brother, he, him, his, son'.lower().replace(\" \", \"\").split(',')\n","        f_words = 'female, woman, girl, sister, she, her, hers, daughter'.lower().replace(\" \", \"\").split(',')\n","    elif type == 3:\n","        #Science/Art\n","        x_words = 'science, technology, physics, chemistry, Einstein, NASA, experiment, astronomy'.lower().replace(\" \", \"\").split(',')\n","        y_words = 'poetry, art, Shakespeare, dance, literature, novel, symphony, drama'.lower().replace(\" \", \"\").split(',')\n","        m_words = 'brother, father, uncle, grandfather, son, he, his, him'.lower().replace(\" \", \"\").split(',')\n","        f_words = '  , mother, aunt, grandmother, daughter, she, hers, her'.lower().replace(\" \", \"\").split(',')\n","    return m_words, f_words, x_words, y_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QBKLSlr9aeLz","colab_type":"code","colab":{}},"source":["import numpy as np\n","from itertools import combinations, filterfalse\n","from sklearn.metrics.pairwise import cosine_similarity\n","from gensim.models.keyedvectors import KeyedVectors\n","import pandas as pd\n","import random\n","import sys\n","import os\n","import pickle\n","random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nn9GGX9sTGOJ","colab_type":"code","colab":{}},"source":["!find /content/ -name '*eigvecs*' | xargs  rm -rf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WV9V25CeMP0n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"executionInfo":{"status":"ok","timestamp":1592310696335,"user_tz":-120,"elapsed":38689,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"9f0faf1b-7890-49ee-b8d8-2b6b7b907a9a"},"source":["def get_bert_embedding(model, wordlist, is_targets=1):\n","    vecss = torch.Tensor().to(device)\n","    for w in wordlist:\n","        text = w + ' is ' + tokenizer.mask_token + '.' if not is_targets else tokenizer.mask_token + ' is ' + w + '.'\n","        vec = tokenizer.prepare_for_model(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)),\n","                                            return_token_type_ids=False, return_tensors='pt')['input_ids'].to(device)\n","        vecs = vec.clone().detach()\n","        # get output\n","        vecs = model.bert(vecs)[0]#[2][nlayer]\n","        vecs = vecs[0][1:-4,:].mean(dim=0).unsqueeze(0) if not is_targets else vecs[0][3:-2,:].mean(dim=0).unsqueeze(0)\n","        vecss = torch.cat((vecss,vecs))\n","    return vecss\n","\n","import densray_bert as bbert\n","def get_eigvecs_dict(layer=-1):\n","    eigvecs_dict = {}\n","    #-1:apply to all layers\n","    if layer==-1:\n","        #'/content/drive/My Drive/negc'+config+'_'+str(nsamples)+'_'+str(l)+'.pt'\n","        #'/content/drive/My Drive/eigvecs_'+config+'_new_'+str(nsamples)+'_'+str(l)+'.pt'\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = ('/content/drive/My Drive/eigvecs_'+config+'_new_'+str(nsamples)+'_'+str(l)+'.pt', True)\n","    elif layer ==-2:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = ('/content/drive/My Drive/eigvecs_'+config+'_new_'+str(nsamples)+'_'+str(l)+'.pt', False)\n","    else:\n","        for l in range(nlayer):\n","            if l==layer:\n","                eigvecs_dict[str(l)] = ('/content/drive/My Drive/eigvecs_'+config+'_new_'+str(nsamples)+'_'+str(l)+'.pt', True)\n","            else:\n","                eigvecs_dict[str(l)] = ('/content/drive/My Drive/eigvecs_'+config+'_new_'+str(nsamples)+'_'+str(l)+'.pt', False)\n","    return eigvecs_dict\n","\n","import pickle\n","for l in range(-2,nlayer):\n","    d = get_eigvecs_dict(l)\n","    df2=open('/content/eigvecs_dict_'+str(l)+'.txt','wb')\n","    pickle.dump(d,df2)\n","    df2.close()\n","\n","def eval_per_layer(layer=-2):\n","    config_class = get_eigvecs_dict(layer)\n","    model = bbert.BertForMaskedLM_1.from_pretrained('bert-'+config+'-uncased', eigvecs_dict=get_eigvecs_dict(l)).to(device)\n","    # turn on eval mode\n","    model.eval()\n","    m = get_bert_embedding(model, m_words, is_targets=0).cpu().detach().numpy()\n","    f = get_bert_embedding(model, f_words, is_targets=0).cpu().detach().numpy()\n","    x = get_bert_embedding(model, x_words, is_targets=1).cpu().detach().numpy()\n","    y = get_bert_embedding(model, y_words, is_targets=1).cpu().detach().numpy()\n","    embed = {}\n","    for i in range(len(m_words)): embed[m_words[i]] = m[i]\n","    for i in range(len(f_words)): embed[f_words[i]] = f[i]\n","    for i in range(len(x_words)): embed[x_words[i]] = x[i]\n","    for i in range(len(y_words)): embed[y_words[i]] = y[i]\n","    return embed\n","\n","\n","\n","for t in range(1,4):\n","    m_words, f_words, x_words, y_words = category_2(t)\n","    l=-2\n","    # no densray\n","    embed = eval_per_layer(layer=l)\n","    d = weat_effect_size(x_words, y_words, m_words, f_words, embed)\n","    p = weat_p_value(x_words, y_words, m_words, f_words, embed, sample=1000)\n","    print(round(d,4),round(p,4),)\n","    #densray\n","    for l in range(-1, 0):\n","        # densray\n","        embed = eval_per_layer(layer=l)\n","        d_densray =  weat_effect_size(x_words, y_words, m_words, f_words, embed)\n","        p_densray = weat_p_value(x_words, y_words, m_words, f_words, embed, sample=1000)\n","        print(round(d_densray,4), round(p_densray,4))\n","    print('\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.3878 0.237\n","0.4015 0.242\n","\n","\n","1.2742 0.004\n","0.3707 0.229\n","\n","\n","0.7759 0.06\n","0.0786 0.439\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p1ITb1cS9KCy","colab_type":"code","colab":{}},"source":["!find /content/ -name '*eigvecs*' | xargs  rm -rf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YPTrrEPDMUCm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"executionInfo":{"status":"ok","timestamp":1592310736332,"user_tz":-120,"elapsed":69568,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"7c45d24d-c50f-4076-891f-e73a0526022d"},"source":["def get_bert_embedding(model, wordlist, is_targets=1):\n","    vecss = torch.Tensor().to(device)\n","    for w in wordlist:\n","        text = w + ' is ' + tokenizer.mask_token + '.' if not is_targets else tokenizer.mask_token + ' is ' + w + '.'\n","        vec = tokenizer.prepare_for_model(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)),\n","                                            return_token_type_ids=False, return_tensors='pt')['input_ids'].to(device)\n","        vecs = vec.clone().detach()\n","        # get output\n","        vecs = model.bert(vecs)[0]#[2][nlayer]\n","        vecs = vecs[0][1:-4,:].mean(dim=0).unsqueeze(0) if not is_targets else vecs[0][3:-2,:].mean(dim=0).unsqueeze(0)\n","        vecss = torch.cat((vecss,vecs))\n","    return vecss\n","\n","import hard_bert as bbert\n","def get_eigvecs_dict(layer=-1):\n","    eigvecs_dict = {}\n","    #-1:apply to all layers\n","    if layer==-1:\n","        #'/content/drive/My Drive/negc'+config+'_'+str(nsamples)+'_'+str(l)+'.pt'\n","        #'/content/drive/My Drive/eigvecs_'+config+'_new_'+str(nsamples)+'_'+str(l)+'.pt'\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = ('/content/drive/My Drive/pc1'+config+'_'+str(nsamples)+'_'+str(l)+'.pt', True)\n","    elif layer ==-2:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = ('/content/drive/My Drive/pc1'+config+'_'+str(nsamples)+'_'+str(l)+'.pt', False)\n","    else:\n","        for l in range(nlayer):\n","            if l==layer:\n","                eigvecs_dict[str(l)] = ('/content/drive/My Drive/pc1'+config+'_'+str(nsamples)+'_'+str(l)+'.pt', True)\n","            else:\n","                eigvecs_dict[str(l)] = ('/content/drive/My Drive/pc1'+config+'_'+str(nsamples)+'_'+str(l)+'.pt', False)\n","    return eigvecs_dict\n","\n","import pickle\n","for l in range(-2,nlayer):\n","    d = get_eigvecs_dict(l)\n","    df2=open('/content/eigvecs_dict_'+str(l)+'.txt','wb')\n","    pickle.dump(d,df2)\n","    df2.close()\n","\n","def eval_per_layer(layer=-2):\n","    config_class = get_eigvecs_dict(layer)\n","    model = bbert.BertForMaskedLM_1.from_pretrained('bert-'+config+'-uncased', eigvecs_dict=get_eigvecs_dict(l)).to(device)\n","    # turn on eval mode\n","    model.eval()\n","    m = get_bert_embedding(model, m_words, is_targets=0).cpu().detach().numpy()\n","    f = get_bert_embedding(model, f_words, is_targets=0).cpu().detach().numpy()\n","    x = get_bert_embedding(model, x_words, is_targets=1).cpu().detach().numpy()\n","    y = get_bert_embedding(model, y_words, is_targets=1).cpu().detach().numpy()\n","    embed = {}\n","    for i in range(len(m_words)): embed[m_words[i]] = m[i]\n","    for i in range(len(f_words)): embed[f_words[i]] = f[i]\n","    for i in range(len(x_words)): embed[x_words[i]] = x[i]\n","    for i in range(len(y_words)): embed[y_words[i]] = y[i]\n","    return embed\n","\n","\n","\n","for t in range(1,4):\n","    m_words, f_words, x_words, y_words = category_2(t)\n","    l=-2\n","    # no densray\n","    embed = eval_per_layer(layer=l)\n","    d = weat_effect_size(x_words, y_words, m_words, f_words, embed)\n","    p = weat_p_value(x_words, y_words, m_words, f_words, embed, sample=1000)\n","    print(round(d,4),round(p,4),)\n","    #densray\n","    for l in range(-1, 0):\n","        # densray\n","        embed = eval_per_layer(layer=l)\n","        d_densray =  weat_effect_size(x_words, y_words, m_words, f_words, embed)\n","        p_densray = weat_p_value(x_words, y_words, m_words, f_words, embed, sample=1000)\n","        print(round(d_densray,4), round(p_densray,4))\n","    print('\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.3878 0.237\n","0.1482 0.391\n","\n","\n","1.2742 0.004\n","-0.1995 0.642\n","\n","\n","0.7759 0.06\n","-0.6103 0.865\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ISbp1YzCbJ-p","colab_type":"text"},"source":["# SEAT"]},{"cell_type":"code","metadata":{"id":"Th3FbIZNbLqi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":580},"executionInfo":{"status":"ok","timestamp":1593584408537,"user_tz":-120,"elapsed":55700,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"8b2b3bd6-3adf-4af9-9c97-ee0c38d4541f"},"source":["!wget https://raw.githubusercontent.com/W4ngatang/sent-bias/master/tests/sent-weat6.jsonl\n","!wget https://raw.githubusercontent.com/W4ngatang/sent-bias/master/tests/sent-weat7.jsonl\n","!wget https://raw.githubusercontent.com/W4ngatang/sent-bias/master/tests/sent-weat8.jsonl"],"execution_count":5,"outputs":[{"output_type":"stream","text":["--2020-07-01 06:20:02--  https://raw.githubusercontent.com/W4ngatang/sent-bias/master/tests/sent-weat6.jsonl\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9880 (9.6K) [text/plain]\n","Saving to: ‘sent-weat6.jsonl’\n","\n","\rsent-weat6.jsonl      0%[                    ]       0  --.-KB/s               \rsent-weat6.jsonl    100%[===================>]   9.65K  --.-KB/s    in 0s      \n","\n","2020-07-01 06:20:02 (55.9 MB/s) - ‘sent-weat6.jsonl’ saved [9880/9880]\n","\n","--2020-07-01 06:20:03--  https://raw.githubusercontent.com/W4ngatang/sent-bias/master/tests/sent-weat7.jsonl\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8703 (8.5K) [text/plain]\n","Saving to: ‘sent-weat7.jsonl’\n","\n","sent-weat7.jsonl    100%[===================>]   8.50K  --.-KB/s    in 0s      \n","\n","2020-07-01 06:20:04 (64.1 MB/s) - ‘sent-weat7.jsonl’ saved [8703/8703]\n","\n","--2020-07-01 06:20:05--  https://raw.githubusercontent.com/W4ngatang/sent-bias/master/tests/sent-weat8.jsonl\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8038 (7.8K) [text/plain]\n","Saving to: ‘sent-weat8.jsonl’\n","\n","sent-weat8.jsonl    100%[===================>]   7.85K  --.-KB/s    in 0s      \n","\n","2020-07-01 06:20:05 (61.5 MB/s) - ‘sent-weat8.jsonl’ saved [8038/8038]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EBD8FmTfbipi","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593584408539,"user_tz":-120,"elapsed":50622,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}}},"source":["import json\n","import numpy as np\n","\n","WEAT_SETS = [\"targ1\", \"targ2\", \"attr1\", \"attr2\"]\n","CATEGORY = \"category\"\n","\n","def load_json(sent_file):\n","    all_data = json.load(open(sent_file, 'r'))\n","    data = {}\n","    for k, v in all_data.items():\n","        examples = v[\"examples\"]\n","        data[k] = examples\n","        v[\"examples\"] = examples\n","    return all_data  # data"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"9M9Wq4jayiZJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593584408951,"user_tz":-120,"elapsed":49981,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}}},"source":["''' Implements the WEAT tests '''\n","import math\n","import itertools as it\n","import numpy as np\n","import scipy.special\n","import scipy.stats\n","\n","# X and Y are two sets of target words of equal size.\n","# A and B are two sets of attribute words.\n","\n","\n","def cossim(x, y):\n","    return np.dot(x, y) / math.sqrt(np.dot(x, x) * np.dot(y, y))\n","\n","\n","def construct_cossim_lookup(XY, AB):\n","    cossims = np.zeros((len(XY), len(AB)))\n","    for xy in XY:\n","        for ab in AB:\n","            cossims[xy, ab] = cossim(XY[xy], AB[ab])\n","    return cossims\n","\n","\n","def s_wAB(A, B, cossims):\n","    return cossims[:, A].mean(axis=1) - cossims[:, B].mean(axis=1)\n","\n","\n","def s_XAB(X, s_wAB_memo):\n","    return s_wAB_memo[X].sum()\n","\n","\n","def s_XYAB(X, Y, s_wAB_memo):\n","    return s_XAB(X, s_wAB_memo) - s_XAB(Y, s_wAB_memo)\n","\n","\n","def p_val_permutation_test(X, Y, A, B, n_samples, cossims, parametric=False):\n","    X = np.array(list(X), dtype=np.int)\n","    Y = np.array(list(Y), dtype=np.int)\n","    A = np.array(list(A), dtype=np.int)\n","    B = np.array(list(B), dtype=np.int)\n","\n","    assert len(X) == len(Y)\n","    size = len(X)\n","    s_wAB_memo = s_wAB(A, B, cossims=cossims)\n","    XY = np.concatenate((X, Y))\n","\n","    if parametric:\n","        s = s_XYAB(X, Y, s_wAB_memo)\n","        samples = []\n","        for _ in range(n_samples):\n","            np.random.shuffle(XY)\n","            Xi = XY[:size]\n","            Yi = XY[size:]\n","            assert len(Xi) == len(Yi)\n","            si = s_XYAB(Xi, Yi, s_wAB_memo)\n","            samples.append(si)\n","\n","        # Compute sample standard deviation and compute p-value by\n","        # assuming normality of null distribution\n","        (shapiro_test_stat, shapiro_p_val) = scipy.stats.shapiro(samples)\n","        sample_mean = np.mean(samples)\n","        sample_std = np.std(samples, ddof=1)\n","        p_val = scipy.stats.norm.sf(s, loc=sample_mean, scale=sample_std)\n","        return p_val\n","\n","    else:\n","        s = s_XAB(X, s_wAB_memo)\n","        total_true = 0\n","        total_equal = 0\n","        total = 0\n","\n","        num_partitions = int(scipy.special.binom(2 * len(X), len(X)))\n","        if num_partitions > n_samples:\n","            # We only have as much precision as the number of samples drawn;\n","            # bias the p-value (hallucinate a positive observation) to\n","            # reflect that.\n","            total_true += 1\n","            total += 1\n","            for _ in range(n_samples - 1):\n","                np.random.shuffle(XY)\n","                Xi = XY[:size]\n","                assert 2 * len(Xi) == len(XY)\n","                si = s_XAB(Xi, s_wAB_memo)\n","                if si > s:\n","                    total_true += 1\n","                elif si == s:  # use conservative test\n","                    total_true += 1\n","                    total_equal += 1\n","                total += 1\n","        else:\n","            for Xi in it.combinations(XY, len(X)):\n","                Xi = np.array(Xi, dtype=np.int)\n","                assert 2 * len(Xi) == len(XY)\n","                si = s_XAB(Xi, s_wAB_memo)\n","                if si > s:\n","                    total_true += 1\n","                elif si == s:  # use conservative test\n","                    total_true += 1\n","                    total_equal += 1\n","                total += 1\n","        return total_true / total\n","\n","\n","def mean_s_wAB(X, A, B, cossims):\n","    return np.mean(s_wAB(A, B, cossims[X]))\n","\n","\n","def stdev_s_wAB(X, A, B, cossims):\n","    return np.std(s_wAB(A, B, cossims[X]), ddof=1)\n","\n","\n","def effect_size(X, Y, A, B, cossims):\n","    X = list(X)\n","    Y = list(Y)\n","    A = list(A)\n","    B = list(B)\n","\n","    numerator = mean_s_wAB(X, A, B, cossims=cossims) - mean_s_wAB(Y, A, B, cossims=cossims)\n","    denominator = stdev_s_wAB(X + Y, A, B, cossims=cossims)\n","    return numerator / denominator\n","\n","\n","def convert_keys_to_ints(X, Y):\n","    return (\n","        dict((i, v) for (i, (k, v)) in enumerate(X.items())),\n","        dict((i + len(X), v) for (i, (k, v)) in enumerate(Y.items())),\n","    )\n","\n","\n","def run_test(encs, n_samples, parametric=False):\n","    X, Y = encs[\"targ1\"][\"encs\"], encs[\"targ2\"][\"encs\"]\n","    A, B = encs[\"attr1\"][\"encs\"], encs[\"attr2\"][\"encs\"]\n","\n","    # First convert all keys to ints to facilitate array lookups\n","    (X, Y) = convert_keys_to_ints(X, Y)\n","    (A, B) = convert_keys_to_ints(A, B)\n","    XY = X.copy()\n","    XY.update(Y)\n","    AB = A.copy()\n","    AB.update(B)\n","    cossims = construct_cossim_lookup(XY, AB)\n","    pval = p_val_permutation_test(X, Y, A, B, n_samples, cossims=cossims, parametric=parametric)\n","    esize = effect_size(X, Y, A, B, cossims=cossims)\n","    return esize, pval"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"VzPRD2WdcWh3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1593584599701,"user_tz":-120,"elapsed":190735,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"17f263e7-826d-4a22-8c01-f2525c062f91"},"source":["method = 'densray' #hard #conceptor\n","if method == 'densray':\n","    import densray_bert as bbert\n","    path = '/content/drive/My Drive/eigvecs_'+config+'_new_'\n","elif method == 'hard':\n","    import hard_bert as bbert\n","    path = '/content/drive/My Drive/pc1'+config+'_'\n","elif method == 'conceptor':\n","    import conceptor_bert as bbert\n","    path = '/content/drive/My Drive/negc'+config+'_'\n","\n","def get_bert_embedding(model, seat_dict):\n","    for weat in WEAT_SETS:\n","        wordlist = seat_dict[weat]['examples']\n","        vecss = {}\n","        for w in wordlist:\n","            vec = tokenizer.prepare_for_model(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(w.lower())),\n","                                              return_token_type_ids=False, return_tensors='pt')['input_ids'].to(device)\n","            vecs = vec.clone().detach()\n","            # get embedding of [CLS] as aggregation of the sentence\n","            vecs = model.bert(vecs)[0][0][0]\n","            vecss[w] = vecs.cpu().detach().numpy()\n","        seat_dict[weat]['encs'] = vecss  \n","    return seat_dict\n","\n","def get_eigvecs_dict(layer=-1):\n","    eigvecs_dict = {}\n","    #-1:apply to all layers, -2 nothing, l only on layer lth.\n","    if layer==-1:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","    elif layer ==-2:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    else:\n","        for l in range(nlayer):\n","            if l==layer:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","            else:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    return eigvecs_dict\n","\n","def eval_per_layer(seat_dict, layer=-2):\n","    model = bbert.BertForMaskedLM_1.from_pretrained('bert-'+config+'-uncased', eigvecs_dict=get_eigvecs_dict(layer)).to(device)\n","    #model = transformers.BertForMaskedLM.from_pretrained('bert-'+config+'-uncased').to(device)\n","    # turn on eval mode\n","    model.eval()\n","    seat_dict = get_bert_embedding(model, seat_dict)#.cpu().detach().numpy()\n","    return seat_dict\n","\n","\n","\n","for c in range(6,9):\n","    seat_dict = load_json('/content/sent-weat'+str(c)+'.jsonl')\n","    l=-2\n","    # no densray\n","    seat_dict = eval_per_layer(seat_dict, layer=l)\n","    d, p = run_test(seat_dict, n_samples=5000, parametric=True)\n","    print(round(d,4),round(p,4),)\n","    #densray\n","    for l in range(-1, 0):\n","        # densray\n","        seat_dict = eval_per_layer(seat_dict, layer=l)\n","        d_densray, p_densray = run_test(seat_dict, n_samples=5000, parametric=True)\n","        print(round(d_densray,4), round(p_densray,4))\n","    print('\\n')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["0.4997 0.0021\n","0.1349 0.2156\n","\n","\n","0.3834 0.0105\n","-0.743 1.0\n","\n","\n","-0.3033 0.9463\n","-0.6633 0.9998\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uekoostwCRBq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1593584787808,"user_tz":-120,"elapsed":378811,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"0272f48b-05b3-4029-f813-30050b2a9be4"},"source":["method = 'hard' #hard #conceptor\n","if method == 'densray':\n","    import densray_bert as bbert\n","    path = '/content/drive/My Drive/eigvecs_'+config+'_new_'\n","elif method == 'hard':\n","    import hard_bert as bbert\n","    path = '/content/drive/My Drive/pc1'+config+'_'\n","elif method == 'conceptor':\n","    import conceptor_bert as bbert\n","    path = '/content/drive/My Drive/negc'+config+'_'\n","\n","def get_bert_embedding(model, seat_dict):\n","    for weat in WEAT_SETS:\n","        wordlist = seat_dict[weat]['examples']\n","        vecss = {}\n","        for w in wordlist:\n","            vec = tokenizer.prepare_for_model(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(w.lower())),\n","                                              return_token_type_ids=False, return_tensors='pt')['input_ids'].to(device)\n","            vecs = vec.clone().detach()\n","            # get embedding of [CLS] as aggregation of the sentence\n","            vecs = model.bert(vecs)[0][0][0]\n","            vecss[w] = vecs.cpu().detach().numpy()\n","        seat_dict[weat]['encs'] = vecss  \n","    return seat_dict\n","\n","def get_eigvecs_dict(layer=-1):\n","    eigvecs_dict = {}\n","    #-1:apply to all layers, -2 nothing, l only on layer lth.\n","    if layer==-1:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","    elif layer ==-2:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    else:\n","        for l in range(nlayer):\n","            if l==layer:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","            else:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    return eigvecs_dict\n","\n","def eval_per_layer(seat_dict, layer=-2):\n","    model = bbert.BertForMaskedLM_1.from_pretrained('bert-'+config+'-uncased', eigvecs_dict=get_eigvecs_dict(layer)).to(device)\n","    #model = transformers.BertForMaskedLM.from_pretrained('bert-'+config+'-uncased').to(device)\n","    # turn on eval mode\n","    model.eval()\n","    seat_dict = get_bert_embedding(model, seat_dict)#.cpu().detach().numpy()\n","    return seat_dict\n","\n","\n","\n","for c in range(6,9):\n","    seat_dict = load_json('/content/sent-weat'+str(c)+'.jsonl')\n","    l=-2\n","    # no densray\n","    seat_dict = eval_per_layer(seat_dict, layer=l)\n","    d, p = run_test(seat_dict, n_samples=5000, parametric=True)\n","    print(round(d,4),round(p,4),)\n","    #densray\n","    for l in range(-1, 0):\n","        # densray\n","        seat_dict = eval_per_layer(seat_dict, layer=l)\n","        d_densray, p_densray = run_test(seat_dict, n_samples=5000, parametric=True)\n","        print(round(d_densray,4), round(p_densray,4))\n","    print('\\n')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["0.4997 0.0023\n","0.0664 0.3518\n","\n","\n","0.3834 0.0114\n","0.3839 0.0096\n","\n","\n","-0.3033 0.9475\n","-0.0303 0.559\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BxldfDCXcKrU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1593584968433,"user_tz":-120,"elapsed":559402,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"045cd3e3-46be-4193-c9c3-519adcad0a2c"},"source":["method = 'conceptor' #hard #conceptor\n","if method == 'densray':\n","    import densray_bert as bbert\n","    path = '/content/drive/My Drive/eigvecs_'+config+'_new_'\n","elif method == 'hard':\n","    import hard_bert as bbert\n","    path = '/content/drive/My Drive/pc1'+config+'_'\n","elif method == 'conceptor':\n","    import conceptor_bert as bbert\n","    path = '/content/drive/My Drive/negc'+config+'_'\n","\n","def get_bert_embedding(model, seat_dict):\n","    for weat in WEAT_SETS:\n","        wordlist = seat_dict[weat]['examples']\n","        vecss = {}\n","        for w in wordlist:\n","            vec = tokenizer.prepare_for_model(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(w.lower())),\n","                                              return_token_type_ids=False, return_tensors='pt')['input_ids'].to(device)\n","            vecs = vec.clone().detach()\n","            # get embedding of [CLS] as aggregation of the sentence\n","            vecs = model.bert(vecs)[0][0][0]\n","            vecss[w] = vecs.cpu().detach().numpy()\n","        seat_dict[weat]['encs'] = vecss  \n","    return seat_dict\n","\n","def get_eigvecs_dict(layer=-1):\n","    eigvecs_dict = {}\n","    #-1:apply to all layers, -2 nothing, l only on layer lth.\n","    if layer==-1:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","    elif layer ==-2:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    else:\n","        for l in range(nlayer):\n","            if l==layer:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","            else:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    return eigvecs_dict\n","\n","def eval_per_layer(seat_dict, layer=-2):\n","    model = bbert.BertForMaskedLM_1.from_pretrained('bert-'+config+'-uncased', eigvecs_dict=get_eigvecs_dict(layer)).to(device)\n","    #model = transformers.BertForMaskedLM.from_pretrained('bert-'+config+'-uncased').to(device)\n","    # turn on eval mode\n","    model.eval()\n","    seat_dict = get_bert_embedding(model, seat_dict)#.cpu().detach().numpy()\n","    return seat_dict\n","\n","\n","\n","for c in range(6,9):\n","    seat_dict = load_json('/content/sent-weat'+str(c)+'.jsonl')\n","    l=-2\n","    # no densray\n","    seat_dict = eval_per_layer(seat_dict, layer=l)\n","    d, p = run_test(seat_dict, n_samples=5000, parametric=True)\n","    print(round(d,4),round(p,4),)\n","    #densray\n","    for l in range(11, 12):\n","        # densray\n","        seat_dict = eval_per_layer(seat_dict, layer=l)\n","        d_densray, p_densray = run_test(seat_dict, n_samples=5000, parametric=True)\n","        print(round(d_densray,4), round(p_densray,4))\n","    print('\\n')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["0.4997 0.0021\n","0.0554 0.3737\n","\n","\n","0.3834 0.0103\n","-0.6035 0.9999\n","\n","\n","-0.3033 0.9475\n","-0.2987 0.9424\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VaPubn2RCY9P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593030904161,"user_tz":-120,"elapsed":898,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"93f498d8-0367-4c71-8a5d-2ed8e36b304c"},"source":["seat_dict[\"targ1\"][\"encs\"]['This is John.'].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(768,)"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"code","metadata":{"id":"6O6CYe-7b_hq","colab_type":"code","colab":{}},"source":["1.0436 0.0\n","-0.0832 0.6804\n","0.3613 0.0205\n","\n","0.1761 0.1465\n","-0.0643 0.652\n","-0.4707 0.9976\n","\n","0.8134 0.0\n","-0.1044 0.7075\n","0.4175 0.0128\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","0.1078 0.2498\n","-0.4781 0.9988\n","-0.1085 0.754\n","\n","0.7247 0.0\n","-0.0976 0.7211\n","0.1075 0.26\n","\n","1.0039 0.0\n","0.3613 0.027\n","0.7547 0.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qyunC-gN-R65","colab_type":"text"},"source":["# CEAT"]},{"cell_type":"code","metadata":{"id":"yA37Wk-MOyBG","colab_type":"code","colab":{}},"source":["#weat  6\n","male = ['John', 'Paul', 'Mike', 'Kevin', 'Steve', 'Greg', 'Jeff', 'Bill']\n","female = ['Amy', 'Joan', 'Lisa', 'Sarah', 'Diana', 'Kate', 'Ann', 'Donna']\n","career = ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career']\n","family = ['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives']\n","\n","#weat 7\n","maths = ['math', 'algebra', 'geometry', 'calculus', 'equations', 'computation', 'numbers', 'addition']\n","arts = ['poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture']\n","male_term = ['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son']\n","female_term = ['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter']\n","\n","#weat 8\n","science = ['science', 'technology', 'physics', 'chemistry', 'Einstein', 'NASA', 'experiment', 'astronomy']\n","arts_8 = ['poetry', 'art', 'Shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama']\n","male_term_8 = ['brother', 'father', 'uncle', 'grandfather', 'son', 'he', 'his', 'him']\n","female_term_8 = ['sister', 'mother', 'aunt', 'grandmother', 'daughter', 'she', 'hers', 'her']\n","\n","w_all = [i.lower() for i in list(set(male+female+career+family+maths+arts+male_term+female_term+science+arts_8+male_term_8+female_term_8))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dFBSjVApUUQT","colab_type":"code","colab":{}},"source":["import pickle\n","f = open('/content/drive/My Drive/ceat_list_5.pkl', 'rb')\n","ceat = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHderYRaUuXK","colab_type":"code","colab":{}},"source":["method = 'densray' #hard #conceptor\n","if method == 'densray':\n","    import densray_bert as bbert\n","    path = '/content/drive/My Drive/eigvecs_'+config+'_new_'\n","elif method == 'hard':\n","    import hard_bert as bbert\n","    path = '/content/drive/My Drive/pc1'+config+'_'\n","elif method == 'conceptor':\n","    import conceptor_bert as bbert\n","    path = '/content/drive/My Drive/negc'+config+'_'\n","\n","\n","def get_eigvecs_dict(layer=-1):\n","    eigvecs_dict = {}\n","    #-1:apply to all layers, -2 nothing, l only on layer lth.\n","    if layer==-1:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","    elif layer ==-2:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    else:\n","        for l in range(nlayer):\n","            if l==layer:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","            else:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    return eigvecs_dict\n","\n","\n","def for_ceat(in_path, out_path, occ=5):\n","\t#import sss as bert\n","\t#model = bbert.BertForMaskedLM_1.from_pretrained('bert-'+config+'-uncased', eigvecs_dict=get_eigvecs_dict(-1)).to(device)\n","    # turn on eval mode\n","\t#model.eval()\n","\t#weat  6\n","\tmale = ['John', 'Paul', 'Mike', 'Kevin', 'Steve', 'Greg', 'Jeff', 'Bill']\n","\tfemale = ['Amy', 'Joan', 'Lisa', 'Sarah', 'Diana', 'Kate', 'Ann', 'Donna']\n","\tcareer = ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career']\n","\tfamily = ['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives']\n","\n","\t#weat 7\n","\tmath = ['math', 'algebra', 'geometry', 'calculus', 'equations', 'computation', 'numbers', 'addition']\n","\tarts = ['poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture']\n","\tmale_term = ['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son']\n","\tfemale_term = ['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter']\n","\n","\t#weat 8\n","\tscience = ['science', 'technology', 'physics', 'chemistry', 'Einstein', 'NASA', 'experiment', 'astronomy']\n","\tarts_8 = ['poetry', 'art', 'Shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama']\n","\tmale_term_8 = ['brother', 'father', 'uncle', 'grandfather', 'son', 'he', 'his', 'him']\n","\tfemale_term_8 = ['sister', 'mother', 'aunt', 'grandmother', 'daughter', 'she', 'hers', 'her']\n","\tw_all = [i.lower() for i in list(set(male+female+career+family+math+arts+male_term+female_term+science+arts_8+male_term_8+female_term_8))]\n","\n","\tdic = {}\n","\tfor word in w_all:\n","\t\tdic[word] = []\n","\t\tfor line in open(in_path):\n","\t\t\tline = tokenizer.tokenize(line.strip().lower())[:510]\n","\t\t\tif word in line:\n","\t\t\t\tindex = line.index(word)\n","\t\t\t\tline = tokenizer.convert_tokens_to_ids(line)\n","\t\t\t\tline = tokenizer.prepare_for_model(line, return_token_type_ids=False, return_tensors='pt')['input_ids'].to(device)\n","\t\t\t\t#get output embedding\n","\t\t\t\t#vec = model(line)[0][0][1:-1][index].cpu().detach().numpy()\n","\t\t\t\tdic[word].append(line)\n","\t\t\tif len(dic[word])>=occ:\n","\t\t\t\tbreak\n","\tf = open(out_path,'wb')\n","\tpickle.dump(dic,f)\n","\tf.close()\n"," \n","\n","for_ceat('/content/drive/My Drive/wikien_senttok.txt','/content/drive/My Drive/ceat_list_100.pkl',100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zJMQ6ZY1Gq0","colab_type":"code","colab":{}},"source":["''' Implements the WEAT tests '''\n","import math\n","import itertools as it\n","import numpy as np\n","import scipy.special\n","import scipy.stats\n","\n","# X and Y are two sets of target words of equal size.\n","# A and B are two sets of attribute words.\n","\n","\n","def cossim(x, y):\n","    return np.dot(x, y) / math.sqrt(np.dot(x, x) * np.dot(y, y))\n","\n","\n","def construct_cossim_lookup(XY, AB):\n","    cossims = np.zeros((len(XY), len(AB)))\n","    for xy in XY:\n","        for ab in AB:\n","            cossims[xy, ab] = cossim(XY[xy], AB[ab])\n","    return cossims\n","\n","\n","def s_wAB(A, B, cossims):\n","    return cossims[:, A].mean(axis=1) - cossims[:, B].mean(axis=1)\n","\n","\n","def s_XAB(X, s_wAB_memo):\n","    return s_wAB_memo[X].sum()\n","\n","\n","def s_XYAB(X, Y, s_wAB_memo):\n","    return s_XAB(X, s_wAB_memo) - s_XAB(Y, s_wAB_memo)\n","\n","\n","def p_val_permutation_test(X, Y, A, B, n_samples, cossims, parametric=False):\n","    X = np.array(list(X), dtype=np.int)\n","    Y = np.array(list(Y), dtype=np.int)\n","    A = np.array(list(A), dtype=np.int)\n","    B = np.array(list(B), dtype=np.int)\n","\n","    assert len(X) == len(Y)\n","    size = len(X)\n","    s_wAB_memo = s_wAB(A, B, cossims=cossims)\n","    XY = np.concatenate((X, Y))\n","\n","    if parametric:\n","        s = s_XYAB(X, Y, s_wAB_memo)\n","        samples = []\n","        for _ in range(n_samples):\n","            np.random.shuffle(XY)\n","            Xi = XY[:size]\n","            Yi = XY[size:]\n","            assert len(Xi) == len(Yi)\n","            si = s_XYAB(Xi, Yi, s_wAB_memo)\n","            samples.append(si)\n","\n","        # Compute sample standard deviation and compute p-value by\n","        # assuming normality of null distribution\n","        (shapiro_test_stat, shapiro_p_val) = scipy.stats.shapiro(samples)\n","        sample_mean = np.mean(samples)\n","        sample_std = np.std(samples, ddof=1)\n","        p_val = scipy.stats.norm.sf(s, loc=sample_mean, scale=sample_std)\n","        return p_val\n","\n","    else:\n","        s = s_XAB(X, s_wAB_memo)\n","        total_true = 0\n","        total_equal = 0\n","        total = 0\n","\n","        num_partitions = int(scipy.special.binom(2 * len(X), len(X)))\n","        if num_partitions > n_samples:\n","            # We only have as much precision as the number of samples drawn;\n","            # bias the p-value (hallucinate a positive observation) to\n","            # reflect that.\n","            total_true += 1\n","            total += 1\n","            for _ in range(n_samples - 1):\n","                np.random.shuffle(XY)\n","                Xi = XY[:size]\n","                assert 2 * len(Xi) == len(XY)\n","                si = s_XAB(Xi, s_wAB_memo)\n","                if si > s:\n","                    total_true += 1\n","                elif si == s:  # use conservative test\n","                    total_true += 1\n","                    total_equal += 1\n","                total += 1\n","        else:\n","            for Xi in it.combinations(XY, len(X)):\n","                Xi = np.array(Xi, dtype=np.int)\n","                assert 2 * len(Xi) == len(XY)\n","                si = s_XAB(Xi, s_wAB_memo)\n","                if si > s:\n","                    total_true += 1\n","                elif si == s:  # use conservative test\n","                    total_true += 1\n","                    total_equal += 1\n","                total += 1\n","        return total_true / total\n","\n","\n","def mean_s_wAB(X, A, B, cossims):\n","    return np.mean(s_wAB(A, B, cossims[X]))\n","\n","\n","def stdev_s_wAB(X, A, B, cossims):\n","    return np.std(s_wAB(A, B, cossims[X]), ddof=1)\n","\n","\n","def effect_size(X, Y, A, B, cossims):\n","    X = list(X)\n","    Y = list(Y)\n","    A = list(A)\n","    B = list(B)\n","\n","    numerator = mean_s_wAB(X, A, B, cossims=cossims) - mean_s_wAB(Y, A, B, cossims=cossims)\n","    denominator = stdev_s_wAB(X + Y, A, B, cossims=cossims)\n","    return numerator / denominator, denominator\n","\n","\n","def convert_keys_to_ints(X, Y):\n","    return (\n","        dict((i, v) for (i, (k, v)) in enumerate(X.items())),\n","        dict((i + len(X), v) for (i, (k, v)) in enumerate(Y.items())),\n","    )\n","\n","\n","def run_test(X,Y,A,B, n_samples, parametric=False):\n","    # First convert all keys to ints to facilitate array lookups\n","    (X, Y) = convert_keys_to_ints(X, Y)\n","    (A, B) = convert_keys_to_ints(A, B)\n","    XY = X.copy()\n","    XY.update(Y)\n","    AB = A.copy()\n","    AB.update(B)\n","    cossims = construct_cossim_lookup(XY, AB)\n","    pval = p_val_permutation_test(X, Y, A, B, n_samples, cossims=cossims, parametric=parametric)\n","    esize, V_i = effect_size(X, Y, A, B, cossims=cossims)\n","    return esize, pval, V_i*V_i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArrRFuQQ_g4o","colab_type":"code","colab":{}},"source":["import random\n","random.seed(42)\n","\n","def random_ids(length, num=10000):\n","    xids,yids,aids,bids = [],[],[],[]\n","    for i in range(num):\n","        xids.append(random.choice(list(range(length))))\n","        yids.append(random.choice(list(range(length))))\n","        aids.append(random.choice(list(range(length))))\n","        bids.append(random.choice(list(range(length))))\n","    return xids,yids,aids,bids\n","\n","xids,yids,aids,bids = random_ids(length=100, num=10000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cYmB9--HnEo8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1593508373570,"user_tz":-120,"elapsed":389089,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"1d7c28f7-4e4e-44ab-c2be-ecd1396c5580"},"source":["import random\n","\n","method = 'densray' #hard #conceptor\n","if method == 'densray':\n","    import densray_bert as bbert\n","    path = '/content/drive/My Drive/eigvecs_'+config+'_new_'\n","elif method == 'hard':\n","    import hard_bert as bbert\n","    path = '/content/drive/My Drive/pc1'+config+'_'\n","elif method == 'conceptor':\n","    import conceptor_bert as bbert\n","    path = '/content/drive/My Drive/negc'+config+'_'\n","#model = bbert.BertForMaskedLM_1.from_pretrained('bert-'+config+'-uncased', eigvecs_dict=get_eigvecs_dict(-1)).to(device)\n","model = transformers.BertForMaskedLM.from_pretrained('bert-'+config+'-uncased').to(device)\n","model.eval()\n","\n","def get_eigvecs_dict(layer=-1):\n","    eigvecs_dict = {}\n","    #-1:apply to all layers, -2 nothing, l only on layer lth.\n","    if layer==-1:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","    elif layer ==-2:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    else:\n","        for l in range(nlayer):\n","            if l==layer:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","            else:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    return eigvecs_dict\n","\n","\n","def load_ceat(path):\n","    f = open(path,'rb')\n","    ceat = pickle.load(f)\n","    f.close()\n","    for word in w_all:\n","        word_t = tokenizer.convert_tokens_to_ids(word)\n","        for i in range(len(ceat[word])):\n","            index = ceat[word][i][0].cpu().detach().tolist().index(word_t)\n","            #get output embedding\n","            ceat[word][i] = model.bert(ceat[word][i])[0][0,index,:].cpu().detach().numpy()\n","    return ceat\n","ceat = load_ceat('/content/drive/My Drive/ceat_list_100.pkl')\n","\n","import tqdm\n","def run_ces(ceat, category, num=10000):\n","    if category==6:\n","        X={wd.lower():ceat[wd.lower()] for wd in male}\n","        Y={wd.lower():ceat[wd.lower()] for wd in female}\n","        A={wd.lower():ceat[wd.lower()] for wd in career}\n","        B={wd.lower():ceat[wd.lower()] for wd in family}\n","    elif category==7:\n","        X={wd.lower():ceat[wd.lower()] for wd in maths}\n","        Y={wd.lower():ceat[wd.lower()] for wd in arts}\n","        A={wd.lower():ceat[wd.lower()] for wd in male_term}\n","        B={wd.lower():ceat[wd.lower()] for wd in female_term}\n","    elif category==8:\n","        X={wd.lower():ceat[wd.lower()] for wd in science}\n","        Y={wd.lower():ceat[wd.lower()] for wd in arts_8}\n","        A={wd.lower():ceat[wd.lower()] for wd in male_term_8}\n","        B={wd.lower():ceat[wd.lower()] for wd in female_term_8}\n","    \n","    d_list = []\n","    p_list = []\n","    V_list = []\n","    for i in range(num):\n","        X = {wd:ceat[wd][xids[i]] for wd in X} #randomly select a vector\n","        Y = {wd:ceat[wd][yids[i]] for wd in Y} #randomly select a vector\n","        A = {wd:ceat[wd][aids[i]] for wd in A} #randomly select a vector\n","        B = {wd:ceat[wd][bids[i]] for wd in X} #randomly select a vector\n","        d, p, V_i = run_test(X,Y,A,B, 1000, True)\n","        d_list.append(d)\n","        p_list.append(p)\n","        V_list.append(V_i)\n","    d_list, p_list, V_list = np.array(d_list), np.array(p_list), np.array(V_list)\n","    W_list = 1/V_list\n","    c = W_list.sum() - (W_list*W_list).sum()/W_list.sum()\n","    Q = (W_list*(d_list*d_list)).sum() - (W_list*d_list).sum()/W_list.sum()\n","    sigma = (Q-(num-1))/c if Q>=(num-1) else 0\n","    v_i = 1/(V_list+sigma)\n","    CES = (v_i*p_list).sum()/v_i.sum()\n","    \n","    pses = np.mean((p_list < 0.05))\n","    pes = np.mean(d_list)\n","    return pes, pses, CES\n","\n","for c in [6,7,8]:\n","    pes, pses, CES = run_ces(ceat, c)\n","    print(pes, pses, CES, '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-1.0308359870264727 0.0001 0.942160388726403 \n","\n","-1.3082747113357986 0.0 0.993377609583838 \n","\n","-1.5655638737829822 0.0 0.9985365274583365 \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WdblnJDZ1GCV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1593508771873,"user_tz":-120,"elapsed":398261,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"eb437490-97ab-45f4-c388-f97ddc604566"},"source":["import random\n","\n","method = 'densray' #hard #conceptor\n","if method == 'densray':\n","    import densray_bert as bbert\n","    path = '/content/drive/My Drive/eigvecs_'+config+'_new_'\n","elif method == 'hard':\n","    import hard_bert as bbert\n","    path = '/content/drive/My Drive/pc1'+config+'_'\n","elif method == 'conceptor':\n","    import conceptor_bert as bbert\n","    path = '/content/drive/My Drive/negc'+config+'_'\n","model = bbert.BertForMaskedLM_1.from_pretrained('bert-'+config+'-uncased', eigvecs_dict=get_eigvecs_dict(-1)).to(device)\n","#model = transformers.BertForMaskedLM.from_pretrained('bert-'+config+'-uncased').to(device)\n","model.eval()\n","\n","def get_eigvecs_dict(layer=-1):\n","    eigvecs_dict = {}\n","    #-1:apply to all layers, -2 nothing, l only on layer lth.\n","    if layer==-1:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","    elif layer ==-2:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    else:\n","        for l in range(nlayer):\n","            if l==layer:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","            else:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    return eigvecs_dict\n","\n","\n","def load_ceat(path):\n","    f = open(path,'rb')\n","    ceat = pickle.load(f)\n","    f.close()\n","    for word in w_all:\n","        word_t = tokenizer.convert_tokens_to_ids(word)\n","        for i in range(len(ceat[word])):\n","            index = ceat[word][i][0].cpu().detach().tolist().index(word_t)\n","            #get output embedding\n","            ceat[word][i] = model.bert(ceat[word][i])[0][0,index,:].cpu().detach().numpy()\n","    return ceat\n","ceat = load_ceat('/content/drive/My Drive/ceat_list_100.pkl')\n","\n","import tqdm\n","def run_ces(ceat, category, num=10000):\n","    if category==6:\n","        X={wd.lower():ceat[wd.lower()] for wd in male}\n","        Y={wd.lower():ceat[wd.lower()] for wd in female}\n","        A={wd.lower():ceat[wd.lower()] for wd in career}\n","        B={wd.lower():ceat[wd.lower()] for wd in family}\n","    elif category==7:\n","        X={wd.lower():ceat[wd.lower()] for wd in maths}\n","        Y={wd.lower():ceat[wd.lower()] for wd in arts}\n","        A={wd.lower():ceat[wd.lower()] for wd in male_term}\n","        B={wd.lower():ceat[wd.lower()] for wd in female_term}\n","    elif category==8:\n","        X={wd.lower():ceat[wd.lower()] for wd in science}\n","        Y={wd.lower():ceat[wd.lower()] for wd in arts_8}\n","        A={wd.lower():ceat[wd.lower()] for wd in male_term_8}\n","        B={wd.lower():ceat[wd.lower()] for wd in female_term_8}\n","    \n","    d_list = []\n","    p_list = []\n","    V_list = []\n","    for i in range(num):\n","        X = {wd:ceat[wd][xids[i]] for wd in X} #randomly select a vector\n","        Y = {wd:ceat[wd][yids[i]] for wd in Y} #randomly select a vector\n","        A = {wd:ceat[wd][aids[i]] for wd in A} #randomly select a vector\n","        B = {wd:ceat[wd][bids[i]] for wd in X} #randomly select a vector\n","        d, p, V_i = run_test(X,Y,A,B, 1000, True)\n","        d_list.append(d)\n","        p_list.append(p)\n","        V_list.append(V_i)\n","    d_list, p_list, V_list = np.array(d_list), np.array(p_list), np.array(V_list)\n","    W_list = 1/V_list\n","    c = W_list.sum() - (W_list*W_list).sum()/W_list.sum()\n","    Q = (W_list*(d_list*d_list)).sum() - (W_list*d_list).sum()/W_list.sum()\n","    sigma = (Q-(num-1))/c if Q>=(num-1) else 0\n","    v_i = 1/(V_list+sigma)\n","    CES = (v_i*p_list).sum()/v_i.sum()\n","    \n","    pses = np.mean((p_list < 0.05))\n","    pes = np.mean(d_list)\n","    return pes, pses, CES\n","\n","for c in [6,7,8]:\n","    pes, pses, CES = run_ces(ceat, c)\n","    print(pes, pses, CES, '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-0.4710406170037398 0.0037 0.7522627975222524 \n","\n","-1.3486192547018008 0.0 0.9947924763352177 \n","\n","-1.5559406969294876 0.0 0.9984912656334195 \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xt-qcM3XeMOD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1593509170278,"user_tz":-120,"elapsed":398385,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"e650f3ed-b5c0-4727-81f2-7aa4aa923602"},"source":["import random\n","\n","method = 'hard' #hard #conceptor\n","if method == 'densray':\n","    import densray_bert as bbert\n","    path = '/content/drive/My Drive/eigvecs_'+config+'_new_'\n","elif method == 'hard':\n","    import hard_bert as bbert\n","    path = '/content/drive/My Drive/pc1'+config+'_'\n","elif method == 'conceptor':\n","    import conceptor_bert as bbert\n","    path = '/content/drive/My Drive/negc'+config+'_'\n","model = bbert.BertForMaskedLM_1.from_pretrained('bert-'+config+'-uncased', eigvecs_dict=get_eigvecs_dict(-1)).to(device)\n","#model = transformers.BertForMaskedLM.from_pretrained('bert-'+config+'-uncased').to(device)\n","model.eval()\n","\n","def get_eigvecs_dict(layer=-1):\n","    eigvecs_dict = {}\n","    #-1:apply to all layers, -2 nothing, l only on layer lth.\n","    if layer==-1:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","    elif layer ==-2:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    else:\n","        for l in range(nlayer):\n","            if l==layer:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","            else:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    return eigvecs_dict\n","\n","\n","def load_ceat(path):\n","    f = open(path,'rb')\n","    ceat = pickle.load(f)\n","    f.close()\n","    for word in w_all:\n","        word_t = tokenizer.convert_tokens_to_ids(word)\n","        for i in range(len(ceat[word])):\n","            index = ceat[word][i][0].cpu().detach().tolist().index(word_t)\n","            #get output embedding\n","            ceat[word][i] = model.bert(ceat[word][i])[0][0,index,:].cpu().detach().numpy()\n","    return ceat\n","ceat = load_ceat('/content/drive/My Drive/ceat_list_100.pkl')\n","\n","import tqdm\n","def run_ces(ceat, category, num=10000):\n","    if category==6:\n","        X={wd.lower():ceat[wd.lower()] for wd in male}\n","        Y={wd.lower():ceat[wd.lower()] for wd in female}\n","        A={wd.lower():ceat[wd.lower()] for wd in career}\n","        B={wd.lower():ceat[wd.lower()] for wd in family}\n","    elif category==7:\n","        X={wd.lower():ceat[wd.lower()] for wd in maths}\n","        Y={wd.lower():ceat[wd.lower()] for wd in arts}\n","        A={wd.lower():ceat[wd.lower()] for wd in male_term}\n","        B={wd.lower():ceat[wd.lower()] for wd in female_term}\n","    elif category==8:\n","        X={wd.lower():ceat[wd.lower()] for wd in science}\n","        Y={wd.lower():ceat[wd.lower()] for wd in arts_8}\n","        A={wd.lower():ceat[wd.lower()] for wd in male_term_8}\n","        B={wd.lower():ceat[wd.lower()] for wd in female_term_8}\n","    \n","    d_list = []\n","    p_list = []\n","    V_list = []\n","    for i in range(num):\n","        X = {wd:ceat[wd][xids[i]] for wd in X} #randomly select a vector\n","        Y = {wd:ceat[wd][yids[i]] for wd in Y} #randomly select a vector\n","        A = {wd:ceat[wd][aids[i]] for wd in A} #randomly select a vector\n","        B = {wd:ceat[wd][bids[i]] for wd in X} #randomly select a vector\n","        d, p, V_i = run_test(X,Y,A,B, 1000, True)\n","        d_list.append(d)\n","        p_list.append(p)\n","        V_list.append(V_i)\n","    d_list, p_list, V_list = np.array(d_list), np.array(p_list), np.array(V_list)\n","    W_list = 1/V_list\n","    c = W_list.sum() - (W_list*W_list).sum()/W_list.sum()\n","    Q = (W_list*(d_list*d_list)).sum() - (W_list*d_list).sum()/W_list.sum()\n","    sigma = (Q-(num-1))/c if Q>=(num-1) else 0\n","    v_i = 1/(V_list+sigma)\n","    CES = (v_i*p_list).sum()/v_i.sum()\n","    \n","    pses = np.mean((p_list < 0.05))\n","    pes = np.mean(d_list)\n","    return pes, pses, CES\n","\n","for c in [6,7,8]:\n","    pes, pses, CES = run_ces(ceat, c)\n","    print(pes, pses, CES, '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-0.7989600833063635 0.001 0.8754349346581141 \n","\n","-1.3528777585733467 0.0 0.9949020566450059 \n","\n","-1.5780045754684953 0.0 0.9986603470779494 \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z83ndlM-5dHO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1593509554433,"user_tz":-120,"elapsed":384130,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"2ba28b2c-f327-43fd-b5af-abe2d52f4e64"},"source":["import random\n","\n","method = 'conceptor' #hard #conceptor\n","if method == 'densray':\n","    import densray_bert as bbert\n","    path = '/content/drive/My Drive/eigvecs_'+config+'_new_'\n","elif method == 'hard':\n","    import hard_bert as bbert\n","    path = '/content/drive/My Drive/pc1'+config+'_'\n","elif method == 'conceptor':\n","    import conceptor_bert as bbert\n","    path = '/content/drive/My Drive/negc'+config+'_'\n","model = bbert.BertForMaskedLM_1.from_pretrained('bert-'+config+'-uncased', eigvecs_dict=get_eigvecs_dict(11)).to(device)\n","#model = transformers.BertForMaskedLM.from_pretrained('bert-'+config+'-uncased').to(device)\n","model.eval()\n","\n","def get_eigvecs_dict(layer=-1):\n","    eigvecs_dict = {}\n","    #-1:apply to all layers, -2 nothing, l only on layer lth.\n","    if layer==-1:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","    elif layer ==-2:\n","        for l in range(nlayer):\n","            eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    else:\n","        for l in range(nlayer):\n","            if l==layer:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', True)\n","            else:\n","                eigvecs_dict[str(l)] = (path+str(nsamples)+'_'+str(l)+'.pt', False)\n","    return eigvecs_dict\n","\n","\n","def load_ceat(path):\n","    f = open(path,'rb')\n","    ceat = pickle.load(f)\n","    f.close()\n","    for word in w_all:\n","        word_t = tokenizer.convert_tokens_to_ids(word)\n","        for i in range(len(ceat[word])):\n","            index = ceat[word][i][0].cpu().detach().tolist().index(word_t)\n","            #get output embedding\n","            ceat[word][i] = model.bert(ceat[word][i])[0][0,index,:].cpu().detach().numpy()\n","    return ceat\n","ceat = load_ceat('/content/drive/My Drive/ceat_list_100.pkl')\n","\n","import tqdm\n","def run_ces(ceat, category, num=10000):\n","    if category==6:\n","        X={wd.lower():ceat[wd.lower()] for wd in male}\n","        Y={wd.lower():ceat[wd.lower()] for wd in female}\n","        A={wd.lower():ceat[wd.lower()] for wd in career}\n","        B={wd.lower():ceat[wd.lower()] for wd in family}\n","    elif category==7:\n","        X={wd.lower():ceat[wd.lower()] for wd in maths}\n","        Y={wd.lower():ceat[wd.lower()] for wd in arts}\n","        A={wd.lower():ceat[wd.lower()] for wd in male_term}\n","        B={wd.lower():ceat[wd.lower()] for wd in female_term}\n","    elif category==8:\n","        X={wd.lower():ceat[wd.lower()] for wd in science}\n","        Y={wd.lower():ceat[wd.lower()] for wd in arts_8}\n","        A={wd.lower():ceat[wd.lower()] for wd in male_term_8}\n","        B={wd.lower():ceat[wd.lower()] for wd in female_term_8}\n","    \n","    d_list = []\n","    p_list = []\n","    V_list = []\n","    for i in range(num):\n","        X = {wd:ceat[wd][xids[i]] for wd in X} #randomly select a vector\n","        Y = {wd:ceat[wd][yids[i]] for wd in Y} #randomly select a vector\n","        A = {wd:ceat[wd][aids[i]] for wd in A} #randomly select a vector\n","        B = {wd:ceat[wd][bids[i]] for wd in X} #randomly select a vector\n","        d, p, V_i = run_test(X,Y,A,B, 1000, True)\n","        d_list.append(d)\n","        p_list.append(p)\n","        V_list.append(V_i)\n","    d_list, p_list, V_list = np.array(d_list), np.array(p_list), np.array(V_list)\n","    W_list = 1/V_list\n","    c = W_list.sum() - (W_list*W_list).sum()/W_list.sum()\n","    Q = (W_list*(d_list*d_list)).sum() - (W_list*d_list).sum()/W_list.sum()\n","    sigma = (Q-(num-1))/c if Q>=(num-1) else 0\n","    v_i = 1/(V_list+sigma)\n","    CES = (v_i*p_list).sum()/v_i.sum()\n","    \n","    pses = np.mean((p_list < 0.05))\n","    pes = np.mean(d_list)\n","    return pes, pses, CES\n","\n","for c in [6,7,8]:\n","    pes, pses, CES = run_ces(ceat, c)\n","    print(pes, pses, CES, '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-1.0035016495250082 0.0 0.944417453453168 \n","\n","-1.3712686272949601 0.0 0.9955850835976079 \n","\n","-1.6322733492374912 0.0 0.9991663835833512 \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Djl-T73F-97E","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YU2xDw5E9q9l","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"moWl_snFV23P","colab_type":"code","colab":{}},"source":["for_ceat('/content/drive/My Drive/wikien_senttok.txt','/content/drive/My Drive/ceat_base_hard.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWzPhkKfjgQp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593514296285,"user_tz":-120,"elapsed":1129,"user":{"displayName":"Liang Sheng","photoUrl":"","userId":"09714697225400851958"}},"outputId":"f4fcdc0c-bc04-483c-d498-8ef556614c28"},"source":["1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"hwVNnq0M2n4k","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}