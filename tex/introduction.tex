Word embeddings, which represent the semantic meaning of text data as vectors, are used as input in natural language processing tasks. It has been found that word embeddings exhibit unexpected social biases, such as gender bias, that are present in their training corpora \citep{bolukbasi2016man, caliskan2017semantics,garg2018word}. An example is that man is associated with computer programmer on the embedding space, and woman is associated with homemaker \citep{bolukbasi2016man}. Contextual word embedding models, such as BERT \citep{devlin2018bert}, have become increasingly common and achieved new state-of-the-art results in the many NLP tasks. Researches have also found gender bias in contextualized embeddings \citep{zhao2019gender,may2019measuring}.

In this work, we aim to eliminate gender information in BERT in a straight-forward and interpretable way. We introduce a debiasing method on BERT using DensRay \citep{dufter2019analytical}, which yields interpretable dimensions by rotating the embedding spaces. We show that gender information is captured in every BERT layer. We apply DensRay to every BERT layer and evaluate two tasks: a set of templates we constructed and the Word Embedding Association Test (WEAT) \citep{caliskan2017semantics}. Our experiments find that the DensRay debiasing method effectively mitigates gender bias, while at the same time maintains the performance of BERT on language modeling and the GLUE tasks \citep{wang2018glue}. As an extension, we also applied this debiasing method to the multilingual-BERT (mBERT) model: we use English gender label for computing the rotation matrix, and debias on our Chinese templates. In summary we contribute: \textbf{i)} We adapt the analytical method DensRay, which was designed for static embeddings, to contextualized embeddings. \textbf{ii)} We demonstrate that DensRay is effective for removing gender information. \textbf{iii)}  We show that the DensRay debiasing method can be applied to mBERT for zero-shot debiasing for other languages.