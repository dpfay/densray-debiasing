Word embeddings, which represent the semantic meaning of
text data as vectors, are used as input in natural language
processing tasks. It has been found that word embeddings
exhibit biases such as gender bias, which are present in their training
corpora \cite{bolukbasi2016man,caliskan2017semantics,garg2018word}. Contextual word
embedding models, such as BERT \cite{devlin2018bert}, have
become increasingly common and achieved new state-of-the-art
results in many NLP tasks. Researchers have also found
gender bias in contextualized
embeddings ~\cite{zhao2019gender,may2019measuring}.

A common approach for removing gender information in static
embeddings is to identify a linear gender subspace (e.g., a
gender direction) and subsequently setting all values on the
gender direction to 0. Successful approaches rely on simple
principal component
analysis \cite{bolukbasi2016man,mu2018all}. \newcite{bolukbasi2016man}
require pairs of gendered words to compute a direction
(e.g., ``man''-``woman'') and \newcite{mu2018all} rely on
computing a PCA of a set of gender words hoping that the
main variation occurs across gender. We propose to use
DensRay \cite{dufter2019analytical}: the main advantage is
that DensRay only requires two or multiple groups of
gendered words. In contrast to \cite{bolukbasi2016man}, it
does not require explicit pairs. Compared
to \cite{mu2018all}, it has explicit supervision with gender
labels. We show in \secref{artexample} that
DensRay is more stable.

In summary, our contributions are: 
i) We adjust DensRay to work on contextualized embeddings.
We apply DensRay to every BERT layer and evaluate two tasks:
a set of templates we constructed and previously used Association Tests. Our experiments find that debiasing with DensRay effectively mitigates gender bias and performs on par with prior approaches. 
ii). We investigate whether debiased models maintain the performance of BERT on language modeling and GLUE \cite{wang2018glue}: we show that DensRay is more robust and interpretable than prior approaches.
iii) We explore how gender information is processed in BERT by applying DensRay to BERT attention heads and layers: we conclude that there is no single attention head responsible for processing gender information.
iv) We show that DensRay can be used to quantify gender bias with interpretable gender scores on token and sentence levels for all representations. 
v) We apply our debiasing method to the multilingual-BERT (mBERT) model: we show that English training data can be used to effectively debias Chinese.

