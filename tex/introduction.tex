Word embeddings, which represent the semantic meaning of text data as vectors, are used as input in natural language processing tasks (Goldberg, 2017). It has disclosed that word embeddings exhibit unexpected social biases, such as gender bias, present in their training corpora \citep{bolukbasi2016man, caliskan2017semantics,garg2018word}. An example is that man is associated with computer programmer on the embedding space, and woman is associated with homemaker \citep{bolukbasi2016man}. Contextual word embedding models, such as BERT \citep{devlin2018bert}, have become increasingly common and achieved new state-of-the-art results in the many NLP tasks. Researches have also found gender bias in contextualized embeddings \citep{zhao2019gender,may2019measuring}.

In this work, we aim to mitigate gender bias on BERT embedding in a straight-forward and interpretable way. We introduce a debiasing method on BERT using the DensRay \citep{dufter2019analytical}, which is a computational method to get the interpretable dimensions by rotating the word embedding spaces. We show that gender information is captured in every BERT layer. We applied DensRay to every BERT layer and evaluated on a set of simple templates we constructed and the Word Embedding Association Test (WEAT) \citep{caliskan2017semantics}, our experiments find that the DensRay debiasing method effectively mitigates gender bias, and do little harm to the performance of the BERT model on language modeling and GLUE tasks \citep{wang2018glue}. As an extension, we also applied this debiasing method to the multilingual-BERT (mBERT) model: we use English gender label for computing the rotation matrix, and debias on our Chinese templates. Our contributions are summarized as the following: 

• We introduce the DensRay debiasing method on BERT, and demonstrates the debiasing effectiveness by our templates and the Word Embedding Association Test.

• We show that the DensRay debiasing method can be applied to mBERT for zero-shot debiasing for other languages.