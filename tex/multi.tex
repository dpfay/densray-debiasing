\subsection{Setup}
As an extension, we apply DensRay to mBERT for zero-shot debiasing on Chinese. Here we use the "bert-multilingual-uncased" model from \citep{wolf2019huggingfaces}, we also use the same setup as the "bert-base-uncased" model in our previous experiments.

As before, we compute the rotation matrices using the English gendered words from the "family" category of the Google analogy test set \citep{mikolov2013efficient}.

Since Chinese is a language that does not contain genus, we can construct the OCCTMP templates by directly translating from the English templates. So we got the following form: \eat{"\text{[MASK]}是一个\textit{occupation}。".} For the occupation name, we referred to Tencent Translation\footnote{https://fanyi.qq.com/} and made some manual adjustments to the translation. After removing the duplicates, we got 302 Chinese templates.

\subsection{Results on OCCTMP}
Results about our experiments on the templates are summarized in \tabref{t:templates3}. Two examples are given in \tabref{t:templates3}. It shows that DensRay trained with English can mitigate gender bias in mBERT: the average difference drops from 0.17 to 0.08 on Chinese templates. Also, the mBERT still gets comparable perplexities on Wikitext-2 after debiasing, see table \tabref{t:ppl2}. 
\begin{table}[ht]
\centering
\footnotesize
\begin{tabular}{lcccc}
\hline
model & prob(he) & prob(she) & diff & var\\
\hline
\scriptsize bert-multi-en 
& 0.51 & 0.14 & 0.36 & 0.06 \\
\scriptsize 
bert-multi-densray-en & 0.33 & 0.12 & 0.21 & 0.03 \\
\scriptsize bert-multi-cn 
& 0.24 & 0.07 & 0.17 & 0.02 \\
\scriptsize bert-multi-densray-cn 
& 0.12 & 0.04 & 0.08 & 0.01\\
\hline
\end{tabular}
\caption{\tablabel{t:templates3}
Results of OCCTMP on mBERT after applied DensRay. Models with \textit{-en} are tested on English templates, and those with \textit{-cn} are tested on Chinese templates.}
\end{table}
\begin{table}[ht]
\centering
\footnotesize
\begin{tabular}{lc}
\hline
model & ppl\\
\hline
bert-multi & 3.58\\
bert-multi-densray & 3.72\\
\hline
\end{tabular}
\caption{\tablabel{t:ppl2}
Language modeling performance on mBERT after applied DensRay.}
\end{table}
\begin{table}[t]
\centering
\footnotesize
\begin{tabular}{llcc}
\hline
sentence & model & \eat{prob(他)} & \eat{prob(她)}\\
\hline
\eat{\text{[MASK]}是一个客座教授。} & bert-multi-en & 0.68 & 0.16\\
& bert-multi-densray-en & 0.51 & 0.18\\
& bert-multi-cn & 0.52 & 0.11\\
 & bert-multi-densray-cn & 0.30 & 0.08\\
\hline
\eat{\text{[MASK]}是一个管理员。} & bert-multi-en & 0.53 & 0.17\\
& bert-multi-densray-en & 0.35 & 0.13\\
& bert-multi-cn & 0.68 & 0.16\\
 & bert-multi-densray-cn & 0.51 & 0.18\\
\hline
\end{tabular}
\caption{\label{t:templates3}
Sanity check on the Chinese templates, where \eat{\textit{他}} means \textit{he} and \eat{\textit{她}} means \textit{she}. The two sentences are translated from \tabref{t:templates2}.}
\end{table}