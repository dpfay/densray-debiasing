\subsection{Setup}
We now show that, in a multilingual contextualized language
model like mBERT,
we can use DensRay  for zero-shot debiasing.
Specifically, we train a DensRay model on English and use it
to debias Chinese.
We use  bert-multilingual-uncased from
\citep{wolf2019huggingfaces}. We  use the same setup as
for bert-base-uncased in our previous experiments. \enote{pd}{If I recall correctly the uncased model is not good for Chinese and only the cased model should be used. This is because they used different preprocessing for both models.}

As before, we compute the rotation matrices using the English gendered words from the ``family'' category of the Google analogy test set \citep{mikolov2013efficient}.

Since Chinese is a language that does not mark gender, we
can construct the OCCTMP templates by directly translating
from the English templates. We use the following form:
``\text{[MASK]}\yin{是一个}\textit{occupation}\yin{。}''
We translate the occupation name based on
Tencent
Translation\footnote{https://fanyi.qq.com/} and make some
manual adjustments to the translation. After removing 
duplicates,  302 Chinese templates remain.

\subsection{Results on OCCTMP}
\tabref{t:templates3} gives
results for the Chinese templates.
Two examples are given in \tabref{t:templates3}. We see that DensRay trained with English can mitigate gender bias in mBERT: the average difference drops from 0.17 to 0.08 on Chinese templates. Also, mBERT still gets comparable perplexities on Wikitext-2 after debiasing: see table \tabref{t:ppl2}. 
\begin{table}[ht]
\centering
\footnotesize
\begin{tabular}{lcccc}
\hline
model & prob(he) & prob(she) & diff & var\\
\hline
\scriptsize bert-multi-en 
& 0.51 & 0.14 & 0.36 & 0.06 \\
\scriptsize 
bert-multi-densray-en & 0.33 & 0.12 & 0.21 & 0.03 \\
\scriptsize bert-multi-cn 
& 0.24 & 0.07 & 0.17 & 0.02 \\
\scriptsize bert-multi-densray-cn 
& 0.12 & 0.04 & 0.08 & 0.01\\
\hline
\end{tabular}
\caption{\tablabel{t:templates3}
Results of OCCTMP on mBERT after applied DensRay. Models with \textit{-en} are tested on English templates, and those with \textit{-cn} are tested on Chinese templates.}
\end{table}
\begin{table}[ht]
\centering
\footnotesize
\begin{tabular}{lc}
\hline
model & ppl\\
\hline
bert-multi & 3.58\\
bert-multi-densray & 3.72\\
\hline
\end{tabular}
\caption{\tablabel{t:ppl2}
Language modeling performance on mBERT after applied DensRay. \enote{pd}{on which language? Not sure whether this table is necessary in the main paper}}
\end{table}
\begin{table}[t]
\centering
\footnotesize
\begin{tabular}{llcc}
\hline
sentence & model & \eat{prob(他)} & \eat{prob(她)}\\
\hline
\eat{\text{[MASK]}是一个客座教授。} & bert-multi-en & 0.68 & 0.16\\
& bert-multi-densray-en & 0.51 & 0.18\\
& bert-multi-cn & 0.52 & 0.11\\
 & bert-multi-densray-cn & 0.30 & 0.08\\
\hline
\eat{\text{[MASK]}是一个管理员。} & bert-multi-en & 0.53 & 0.17\\
& bert-multi-densray-en & 0.35 & 0.13\\
& bert-multi-cn & 0.68 & 0.16\\
 & bert-multi-densray-cn & 0.51 & 0.18\\
\hline
\end{tabular}
\caption{\label{t:templates3}
Sanity check on the Chinese templates, where \eat{\textit{他}} means \textit{he} and \eat{\textit{她}} means \textit{she}. The two sentences are translated from \tabref{t:templates2}.}
\end{table}
