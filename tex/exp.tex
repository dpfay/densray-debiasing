\subsection{Setup and Data}
In the experiments we downcase all  text  and use the BERT models ``bert-base-uncased" and ``bert-large-uncased". We implemented all experiments using the transformers library \shortcite{wolf2019huggingfaces}.

To compute the rotation matrices by DensRay, we need the
labels of a gendered word list and a corpus. For the word
list, we get 23 masculine words and 23 feminine words from
the ``family''
category,\footnote{http://download.tensorflow.org/data/questions-words.txt}
of the Google analogy test set \shortcite{mikolov2013efficient}
and label them as 1 and -1. As the input corpus, we collect
text data from Wikipedia that contains 5,000 (resp.\ 10,000)
occurrences of words in the gendered list for the BERT base
(resp.\ large) model. We carefully balance the occurrences such that the number of male and female samples are equal. We set  $\alpha_{\neq}=\alpha_{=}=0.5$, as we have balanced the training samples from the corpus.

We compare with the hard debiasing method \cite{mu2018all} and the debiasing conceptor \cite{karve2019conceptor} to eliminate gender bias as
adapted to contextualized embeddings by \shortcite{karve2019conceptor}. 

\subsection{Evaluations}
\subsubsection{OCCTMP}
We use two evaluation datasets
to measure gender
bias: WEAT (Section~\ref{sec:weat}) and OCCTMP.

OCCTMP is a new evaluation dataset
based on occupation templates
that we created specifically
for
the evaluation of contextualized language models.  It has
the added advantage that results are easier to interpret
than those for WEAT.

To construct OCCTMP,
we start with 
320 occupation
names\footnote{https://github.com/tolga-b/debiaswe/blob/master/data/professions.json}
provided by \cite{bolukbasi2016man}.
Each occupation name is converted into a template of the form
``[MASK] is an \textit{occupation}.''
We measure
gender bias
in the templates as the average difference
between the probability of BERT predicting [MASK] as ``he''
vs.\ ``she''
\begin{eqnarray}
\text{diff}=\frac{1}{|{\mathcal T}|} \sum_{T \in
	{\mathcal T}}(p(\mbox{he}| T) - p(\mbox{she}|T))\nonumber
\end{eqnarray}
where $\mathcal T$ is the set of 320 templates.  We find
that for most experiments and most templates the probability
of ``he'' is higher than ``she'', which qualitatively
indicates that gender bias can be identified using these templates. We
also find that in most cases the sum of the two probability
is higher than 0.7; thus, this evaluation task is a good fit
for BERT because it has learned that a pronoun is likely to
occur in the masked position.
Our templates  can be easily extended to other
languages
as we later show for
Chinese.
\subsubsection{Association Tests}\label{sec:weat}
An alternative way to measure gender bias is based on association tests, which originated from sociological research. \cite{greenwald1998measuring} proposed the Implicit Association Test (IAT) to quantified societal bias. In IAT, response times were recorded when subjects were asked to match two concepts. For example, subjects were asked to match black and white names with “pleasant” and “unpleasant” words. Subjects tended to have shorter response times for concepts they thought associated. 

Based on IAT, \cite{caliskan2017semantics} proposed the Word Embedding Association Test (WEAT), which used word similarities between targets and attributes instead of the response times to get rid of the requirement of human subjects. Consider two sets of target words $X_1,X_2$ with equal size $|X_1|=|X_2|$, and two sets of attribute words $A_1,A_2$. The null hypothesis in the statistical test of WEAT is: there is no difference in the similarity between $X_1,X_2$ and $A_1,A_2$. In the prior literature it has been argued that if the null hypothesis can't be rejected, there is no significant gender bias. 
\enote{pd}{we
	should critisize this reasoning. The null and alternative
	hypothesis should be swapped. Has other work criticized
	this setup? Maybe we can do the test in addition in an
	alternative way?} \enote{sl}{as far as I see, all the association tests(IAT,WAET,SEAT,CEAT) use this hypothesis setup, I think it's just a 'tradition'.} 
The WEAT test statistic is defined as
\begin{eqnarray}
s(X_1,X_2,A_1,A_2)=\sum_{x\in X_1}s(x,A_1,A_2)\nonumber\\
-\sum_{x\in X_2}s(x,A_1,A_2),\nonumber
\end{eqnarray}
\enote{pd}{I do not understand the notation fully. Is the p-value computed with respect to a single partition $i$?}\enote{sl}{the probability is computed over the space of partitions}
where
\begin{eqnarray}
s(x,A_1,A_2)=\mbox{mean}_{a\in A_1}cos(\vec{x},\vec{a})-\mbox{mean}_{a\in A_2}cos(\vec{x},\vec{a})\nonumber
\end{eqnarray}
in which $cos(\vec{x},\vec{a})$ denotes the cosine similarity between embedding vector $\vec{x}$ and $\vec{a}$. Intuitively, $s(x,A_1,A_2)$ measures the association of a word with the attributes, so the test statistic measures the differential association of the two target sets with the attributes. 

Let $\{({X_1}_i,{X_2}_i)\}_{i}$ denote all the partitions of $X_1\cup X_2$. The one-sided $p$-value of the permutation test is defined as $$p=Pr_i[s({X_1}_i,{X_2}_i,A_1,A_2)>s(X_1,X_2,A_1,A_2)]$$
The effect size $d$-value is a normalized measure of how separated the two distributions of associations between the target and attribute are. It is defined as
\begin{eqnarray}
d=\frac{s(X_1,X_2,A_1,A_2)}{std_{x\in X_1 \cup X_2}s(x,A_1,A_2)}.\nonumber
\end{eqnarray}

To extend WEAT to contextual embeddings, \cite{karve2019conceptor} extracted contextual embeddings from the template '[MASK] is \textit{word}.' \cite{may2019measuring} proposed Sentence Embedding Association Test (SEAT), which designed more complex templates to extract word embeddings. 

Dispensed with templates, \cite{guo2020detecting} proposed Contextualized Embedding Association Test (CEAT), which extracted the embeddings of the stimulus' occurrences from the corpus, and computed the weighted mean of effect sizes and statistical significance by a random-effects model. The combination effect size is
\begin{eqnarray}
d_c(X_1,X_2,A_1,A_2)=\frac{\sum_{i=1}^{N}v_id_i}{\sum_{i=1}^{N}v_i}\nonumber
\end{eqnarray} 
where $v_i$ is the weights in the random-effects model. To measure the statistical significance, they used two-tailed $p$-value $p_c = 2[1-\phi(|\frac{d_c}{std(d_c)}|)]$.

\enote{hs}{i think there is a summary sentence missing here:
	how do we use this to evaluate / compare debiasing methods?}

\subsubsection{Model Performance}
It is crucial that debiasing methods do not harm downstream
performance of BERT models. Thus we test the perplexity of
language modeling on Wikitext-2
\cite{merity2016pointer}, a subset of Wikipedia with 2
million words. We also test on GLUE tasks
\cite{wang2018glue}. For all the tests we follow the same
setup as
\cite{wolf2019huggingfaces}.\footnote{https://huggingface.co/transformers/}