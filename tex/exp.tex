\subsection{Experiments Setup}
In the experiments we use the BERT models "bert-base-uncased" and "bert-large-uncased" from the huggingface trasnformers library \citep{wolf2019huggingfaces}.

To compute the rotation matrices by DensRay, there needs a the gendered word list as label, and an input corpus to get the embeddings from BERT. For the word list, we get 23 masculine words and 23 feminine words from the "family" category\footnote{http://download.tensorflow.org/data/questions-words.txt} of the Google analogy test set \citep{mikolov2013efficient}, and label them as 1 and -1. As the input corpus,  for BERT base (large) model we collect text data from Wikipedia that contains 5,000 (10,000) occurrences of words in the gendered list, in which  the number of masculine and feminine samples are equal. The hyperparameters are set to $\alpha_{\neq}=\alpha_{=}=0.5$, since we have balanced the training samples from the corpus.

\subsection{Results on Templates}
Results about our experiments on the templates are summarized in table \ref{t:templates1}. Two example templates are given in table \ref{t:templates2}. The evaluation on our templates shows that DensRay can mitigate the gender bias on BERT.
\begin{table*}[ht]
\centering
\begin{tabular}{lllll}
\hline
model & prob(he) & prob(she) & diff & var\\
\hline
bert-base & 0.6594 & 0.1874 & 0.4720 & 0.1600 \\
bert-base-densray & 0.5106 & 0.3447 & {0.1658} & 0.0119\\
\hline
bert-large  & 0.6287 & 0.1907 & 0.4380 & 0.1262 \\
bert-large-densray  & 0.4751 & 0.2923 & {0.1827} & 0.0150\\
\hline
\end{tabular}
\caption{\label{t:templates1}
BERT debiasing results on templates. \textit{bert-base} and \textit{bert-large} are the original model without debiasing. \textit{prob(he)} is the mean probability that model predict \textit{he} as the [MASK]in all templates. \textit{var} is the variance of the differences between the probability of BERT predicts [MASK] as \textit{he} and \textit{she}.}
\end{table*}
\begin{table*}[ht]
\centering
\begin{tabular}{llll}
\hline
sentence & model & prob(he) & prob(she)\\
\hline
[MASK] is a adjunct professor. & bert-base & 0.7231 & 0.1942\\
 & bert-base-densray & 0.4423 & 0.4740\\
 & bert-large & 0.7181 & 0.2212\\
 & bert-large-densray & 0.3974 & 0.5316\\
\hline
[MASK] is a administrator. & bert-base & 0.6296 & 0.2337\\
 & bert-base-densray & 0.5045 & 0.3762\\
 & bert-large & 0.6456 & 0.2269\\
 & bert-large-densray & 0.4536 & 0.3716\\
\hline
\end{tabular}
\caption{\label{t:templates2}
Sanity check on the templates.}
\end{table*}

\subsection{Results on WEAT}
In WEAT we measure the effect size $d$-value and the oneside $p$-value of the permutation test. A higher absolute value of the $d$-value indicates larger gender bias between the target words with respect to the attribute words. So, for the $d$-value, the closer to zero, the less gender bias. Refer to the definition of the null hypothesis, if the $p$-value is less than 0.05 we will reject the null hypothesis so that there will be a significant gender bias. So, we would prefer a high $p$-value (at least 0.05) to indicate the lack of gender bias. Follow the same WEAT word lists setup as \citet{karve2019conceptor}, the results on WEAT is shown on table \ref{t:weat1}. For all the three categories, DensRay decreased absolute value of $d$-value and increased the $p$-value, although on bert-large still showd strong bias in \textit{(Career, Family) vs (Male, Female)}.
\begin{table*}[ht]
\centering
\begin{tabular}{llll}
\hline
category & model & d & p\\
\hline
(Career, Family) vs (Male, Female) & bert-base & 0.6581 & 0.08 \\
                  & bert-base-densray & 0.6397 & 0.11\\
                  & bert-large & 1.5705 & 0.00^{*} \\
                  & bert-large-densray & 0.9980 & 0.02^{*}\\
\hline
(Math, Arts) vs (Male, Female) & bert-base & 0.6017 & 0.11 \\
                  & bert-base-densray & 0.0739 & 0.45\\
                  & bert-large & 0.2239 & 0.35 \\
                  & bert-large-densray & -0.0145 & 0.48\\
\hline
(Science, Arts) vs (Male, Female) & bert-base & 0.7762 & 0.08 \\
                  & bert-base-densray & 0.0167 & 0.49\\
                  & bert-large & 0.816 & 0.04^{*}  \\
                  & bert-large-densray & 0.6743 & 0.10\\
\hline
\end{tabular}
\caption{\label{t:weat1}
BERT debiasing results on WEAT. Number with * shows significant gender bias.}
\end{table*}

\subsection{Impact on BERT Model}
Here we want to evaluate the performance of BERT model after applied DensRay. We test the perplexity of language modeling on Wikitext-2 dataset \citep{merity2016pointer} which is a subset of Wikipedia with 2 million words, the results in table \ref{t:ppl1} show that DensRay caused a small increase in perplexity on Wikitext-2 for both BERT base and large model.
\begin{table}[ht]
\centering
\begin{tabular}{llll}
\hline
model & ppl\\
\hline
bert-base & 3.7714\\
bert-base-densray & 3.8051\\
\hline
bert-large & 3.2928\\
bert-large-densray & 3.3503\\
\hline
\end{tabular}
\caption{\label{t:ppl1}
Language modeling performance on BERT after applied DensRay.}
\end{table}

Follow the same setup as \citet{wolf2019huggingfaces}\footnote{https://huggingface.co/transformers/}, we also evaluate on the GLUE tasks \citep{wang2018glue}, results are summarized in table \ref{t:glue1}. 
\begin{table*}[ht]
\centering
\begin{tabular}{llllllllll}
\hline
model & CoLA &SST-2&MRPC&STS-B&QQP&MNLI&QNLI&RTE&WNLI\\
\hline
bert-base & 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714\\
bert-base-densray & 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714\\
\hline
bert-large & 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714\\
bert-large-densray & 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714& 3.7714\\
\hline
\end{tabular}
\caption{\label{t:glue1}
GLUE tasks performance on BERT after applied DensRay.}
\end{table*}

\subsection{Discussions}
\subsubsection{the impact of training samples}
Through evaluation and inspection of the impact on the performance of downstream tasks, experiments show that DensRay is an effective debiasing method on BERT. Although DensRay is an analytical solution, the effect still depends on the label data. In the experiments, we regarded the occurrences of the same word in the corpus as independent words with the same gender label, and used balanced samples for masculine and feminine words. Now we analyze the impact of these processes.

Since there are only 46 words in the rendered word list, if we average their embedding under different contexts, there will be only 46 training samples left for DensRay to calculate. DensRay is essentially a supervised learning method. In the case of insufficient labels, it is difficult for supervised learning to extract useful features. Treating different occurrences as different words greatly enriches training samples. As shown in figure, the debiasing results improved when we increase the number of training samples.

The same as other projection-based debiasing methods \citep{bolukbasi2016man,zhao2019gender,dev2019attenuating, karve2019conceptor}, the premise of DensRay debiasing is that the bias direction should be correct. If the sample is unbalanced, the bias direction calculated by DensRay will be biased towards either the male or the female, resulting in deleting the gender subspace during debiasing will reverse the gender bias (e.g. there are more masculine words in unbalanced text data, thus the embeddings will be biased towards female after biased).The figure also shows that balanced training sample improved the debiasing performers. 
\begin{figure*}
    \centering
    \includegraphics{}
    \caption{Here should be a graph.}
    \label{fig:my_label}
\end{figure*}

\subsubsection{the ways of debiasing}
In this experiment, we used the method of removing the first dimension (replacing its value by $0$) of the gender interpreteble subspace to remove gender bias. Here we explore some other ways.

In figure \ref{fig:meandebias}, we tried two other ways to remove bias. The first is to replace the first dimension of the gender interpreteble subspace with the mean value of the first dimension of the training samples. The second way is to standardize the first dimension. The results showed that both of these methods did not perform well. We further checked the mean and found that the mean of the different layers is not stable around 0, which is a problem worthy for further exploring.
\begin{figure*}
    \centering
    \includegraphics{}
    \caption{Here should be a graph.}
    \label{fig:meandebias}
\end{figure*}

As shown in figure \ref{moredim}, we try to delete more dimensions. The results show that removing more dimensions does not improve the debiasing results significantly.
\begin{figure*}
    \centering
    \includegraphics{}
    \caption{Here should be a graph.}
    \label{fig:moredim}
\end{figure*}

\subsubsection{debiasing on each BERT layer}
Here we only apply DesnRay on one BERT layer at a time. We constructed a table to illustrate the top three layers with the best performance on our templates and the three WEAT categories.
\begin{table*}[ht]
\centering
\begin{tabular}{llllllllll}
\hline
\end{tabular}
\caption{\label{t:bestlayers}
Here needs a table.}
\end{table*}

