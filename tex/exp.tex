\subsection{Setup and Data}
In the experiments, we lowercase all text and use the BERT models ``bert-base-uncased" and ``bert-large-uncased". We implemented all experiments using the transformers library \cite{wolf2019huggingfaces}.

To compute the rotation matrices by DensRay, we need the
labels of a gendered word list and a corpus. For the word
list, we get 23 masculine words and 23 feminine words from
the ``family''
category,\footnote{http://download.tensorflow.org/data/questions-words.txt}
of the Google analogy test set \cite{mikolov2013efficient}. As the input corpus, we collect text data from Wikipedia that contains 5,000 (resp.\ 10,000)
occurrences of words in the gendered list for the BERT base
(resp.\ large) model. We carefully balance the occurrences such that the number of male and female samples are equal. We set  $\alpha_{\neq}=\alpha_{=}=0.5$, as we have balanced the training samples from the corpus.

We applied DensRay to all BERT layers for debiasing. We
compare with the hard debiasing method \cite{mu2018all} and
the debiasing conceptor \cite{karve2019conceptor} to
eliminate gender bias as adapted to contextualized
embeddings by \cite{karve2019conceptor}. In the experiments, we found that hard debiasing applied to all BERT layers
yields better results than only applying it to the last layer. In contrast, when applying debiasing conceptor
to all layers, language modeling performance (as measured by
perplexity) is extremely poor.
We therefore applied the
debiasing conceptor only to the last BERT layer.

\subsection{Evaluation}
We use two evaluation datasets to measure gender bias: Association tests (Section~\ref{sec:weat}) and OCCTMP.
OCCTMP is a new evaluation dataset based on occupation templates
that we created specifically to evaluate contextualized language models.  It has the added advantage that results are easier to interpret than those for Association tests.

\subsubsection{OCCTMP}
To construct OCCTMP, we start with 320 occupation names %\footnote{https://github.com/tolga-/debiaswe/blob/master/data/professions.json} 
provided by \newcite{bolukbasi2016man}. Each occupation name is converted into the format ``[MASK] is an \textit{occupation}.''
We measure gender bias as the average difference between the log probability of BERT predicting [MASK] as ``he'' vs.\ ``she''
%\begin{eqnarray}
\begin{gather*}
\text{diff}=\frac{1}{|{\mathcal T}|} \sum_{T \in
	{\mathcal T}}(\log p_{max} - \log p_{min}),\\
\mbox{where }p_{max}=\max\{p(\mbox{he}| T),p(\mbox{she}| T)\} \mbox{; }p_{min}=\min\{p(\mbox{he}| T),p(\mbox{she}| T)\}
\end{gather*}
%\end{eqnarray}
where $\mathcal T$ is the set of 320 templates.  We find
that for most experiments and templates, the probability of
``he'' is higher than ``she'', which qualitatively indicates
that gender bias can be identified using the OCCTMP templates. We also find that in most cases the sum of the two probability is higher than 0.7; thus, this evaluation task is a good fit for BERT because it has learned that a pronoun is likely to occur in the masked position. Our templates can be easily extended to other languages such as Chinese.

\subsubsection{Association Tests}\label{sec:weat}
Association tests originated in sociological research. \newcite{greenwald1998measuring} proposed the Implicit Association Test (IAT) to quantify societal bias. In IAT, response times were recorded when subjects were asked to match two concepts. For example, subjects were asked to match black and white names with “pleasant” and “unpleasant” words. Subjects tended to have shorter response times for concepts they thought associated. 

Based on IAT, \newcite{caliskan2017semantics} proposed the
Word Embedding Association Test (WEAT). It uses word
similarities between targets and attributes instead of the
response times to get rid of the requirement for human
subjects. Consider two sets of target words $X_1,X_2$ with
equal size $|X_1|=|X_2|$, and two sets of attribute words
$A_1,A_2$. The null hypothesis in the WEAT statistical test
is: there is no difference in the similarity between
$X_1,X_2$ and $A_1,A_2$. In the prior literature, it has
been argued that if the null hypothesis cannot be rejected,
there is no significant gender bias.  The WEAT test
statistic is defined as
\begin{gather*}
s(X_1,X_2,A_1,A_2)=\sum_{x\in X_1}s(x,A_1,A_2)-\sum_{x\in X_2}s(x,A_1,A_2),\\
\mbox{where } s(x,A_1,A_2)=\mbox{mean}_{a\in A_1}cos(\vec{x},\vec{a})-\mbox{mean}_{a\in A_2}cos(\vec{x},\vec{a})
\end{gather*}
in which $cos(\vec{x},\vec{a})$ denotes the cosine similarity between embedding vector $\vec{x}$ and $\vec{a}$. Intuitively, $s(x,A_1,A_2)$ measures the association of a word with the attributes, so the test statistic measures the differential association of the two target sets with the attributes. 

Let $\{({X_1}_i,{X_2}_i)\}_{i}$ denote all the partitions of $X_1\cup X_2$. The one-sided $p$-value of the permutation test is defined as $$p=Pr_i[s({X_1}_i,{X_2}_i,A_1,A_2)>s(X_1,X_2,A_1,A_2)]$$
The effect size $d$-value is a normalized measure of how separated the two distributions of associations between the target and attribute are. It is defined as
\begin{eqnarray}
d=\frac{s(X_1,X_2,A_1,A_2)}{\mbox{std}_{x\in X_1 \cup X_2}s(x,A_1,A_2)}.\nonumber
\end{eqnarray}

To extend WEAT to contextual embeddings,
\newcite{karve2019conceptor} extracted contextual embeddings
from the template ``[MASK] is \textit{word}''.
\newcite{may2019measuring} proposed the Sentence Embedding
Association Test (SEAT), for which they designed more complex templates to extract word embeddings. 

%Dispensed with templates, \cite{guo2020detecting} proposed the %Contextualized Embedding Association Test (CEAT), which extracted %the embeddings of the stimulus' occurrences from the corpus, and %computed the weighted mean of effect sizes and statistical %significance by a random-effects model. The combination effect size %is
%\begin{eqnarray}
%d_c(X_1,X_2,A_1,A_2)=\frac{\sum_{i=1}^{N}v_id_i}{\sum_{i=1}^{N}v_i}\%nonumber
%\end{eqnarray} 
%where $v_i$ is the weights in the random-effects model. Two-tailed %$p$-value is used to measure the statistical significance:
%\begin{eqnarray}
%	p_c = 2[1-\phi(|\frac{d_c}{std(d_c)}|)]\nonumber
%\end{eqnarray}
%
In these association tests, we measure the effect size $d$-value and the
one-sided $p$-value of the permutation test.  A $d$-value closer to zero indicates less gender bias.  We also prefer a high $p$-value (at least 0.05) that aims to not reject the null hypothesis, i.e., we do not reject that there is no gender bias. We use the three categories C6: career/family, C7: math/arts, C8: science/arts, following \newcite{karve2019conceptor}'s WEAT setup and \newcite{may2019measuring}'s SEAT setup.

\subsubsection{Model Performance}
It is crucial that debiasing methods do not harm the downstream
performance of BERT models. Thus we test the perplexity of language modeling on Wikitext-2 \cite{merity2016pointer}, a subset of Wikipedia with 2 million words. We also test on GLUE  \cite{wang2018glue}, using the same setup as \cite{wolf2019huggingfaces}.%\footnote{https://huggingface.co/transformers/}
