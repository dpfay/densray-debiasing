\subsection{Adapting DensRay to Contextualized Language Models}
We now describe how we adapt DensRay to contextualized
language models. Given a set of gendered words
$V$, we extract sentences containing a word in $V$ from a
corpus. We run a contextualized language model
with $M$ layers
on each
sentence
$t_1,t_2,\ldots,v,\ldots t_{n-1},t_n$ (where $v \in V$)
and compute the contextualized representations $e^m, 1\leq m
\leq M$ of $v$, one for each layer. 
We compute an orthogonal rotation
matrix $Q_m$ for the $m$th BERT layer using Eq.\
\ref{eq:densray2}.
Finally, we set the dimensions
of the gender subspace to $0$ with the goal of eliminating
or at least reducing
gender
information that may cause bias. In this
paper, we take the first dimension of the rotated space as
the gender subspace.

\subsection{Evaluation}\label{sec:eval}
We use two evaluation datasets
to measure gender
bias: WEAT (Section~\ref{sec:weat}) and OCCTMP.

OCCTMP is an evaluation dataset
based on occupation templates
that
is tailored for 
the evaluation of contextualized language models.  It has
the added advantage that results are easier to interpret
than the measures used for WEAT.

To construct OCCTMP,
we start with 
320 occupation
names\footnote{https://github.com/tolga-b/debiaswe/blob/master/data/professions.json}
provided by \citet{bolukbasi2016man}.
Each occupation name is converted into a template of the form
``[MASK] is an \textit{occupation}.''
We measure
gender bias
in the templates as the average difference
between the probability of BERT predicting [MASK] as ``he''
vs.\ ``she''
\begin{eqnarray}
    \text{diff}=frac{1}{|{\mathcal T}|} \sum_{T \in
      {\mathcal T}}(p(\mbox{he}| T) - p(\mbox{she}|T)\nonumber
\end{eqnarray}
Throughout the paper, we find that for most templates the probability of ``he'' is higher than ``she'', which qualitatively indicates that gender bias exists in these templates. We also find that for most sentences the sum of the two probability is higher than 0.7, which means that the predictions will be stable. This templates set can be easily extended to other languages that do not have gendered profession names, like Chinese.
