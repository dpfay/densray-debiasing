\subsection{DensRay debiasing on BERT}
Here we introduce our methodology to apply DensRay on BERT. With a given gendered word list $V:=\{v_1,v_2,\dots,v_n\}$, we put the sentences which contains the gendered words into BERT model without masking. Due to the different contexts, for each word we regard every occurrence in a corpus $v_{i_j}$ of the word $v_i$ has an independence embedding $e_{i_j}$. For each sentence contains word $v_{i_j}$, the output of each BERT layer yields a contextual embedding $e_{i_j}^m,m\in\{1,2,\dots,M\}$, where $M$ is the number of layers in the BERT model. Then an orthogonal rotation matrix $Q_m$ can be computed by formula \ref{eq:densray2}.

We believe that by identifying the interpretable gender subspace and replacing its parameters by $0$, the gender information that may cause bias can be mitigated. When a new sentence or corpus is sent into the inference process of BERT, we can rotate its original embedding on the output of the first BERT layer $E_1$ by the pre-computed $Q_m$ to an interpretable embedding $\hat{E_1}:=E_1Q_1$. Then we replace the first dimension of $\hat{E_1}$ by $0$ and rotate it back to get a new $E_1:=\hat{E_1}Q_1^T$, since $Q^{-1}=Q^T$ for orthogonal matrix $Q$. This operation can be applied after every layer in the inference process to yield a debiased BERT output. 

\subsection{Evaluations}\label{sec:eval}
In this paper we will use WEAT to measure gender bias. Besides, to quickly test the results of debiasing, we also constructed a set of templates with 320 occupation names\footnote{https://github.com/tolga-b/debiaswe/blob/master/data/professions.json} provided by \citet{bolukbasi2016man} for masked language modeling "[MASK] is a \textit{occupation}." The gender bias in the templates is measured by the average difference between the probability of BERT predicts [MASK] as "he" and "she",
\begin{eqnarray}
    diff=\mathop{mean}_{i \in templates}(\mathop{prob}(he)-\mathop{prob}(she))\nonumber
\end{eqnarray}
Through case study, we find that for most templates the probability of "he" is higher than "she", which qualitatively indicates that gender bias exists in these templates. We also find that for most sentences the sum of the two probability is higher than 0.7, which means that the predictions will be stable. This templates set can be easily extended to other languages that don't have genus.
