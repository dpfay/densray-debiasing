\subsection{DensRay debiasing on BERT}
While DensRay has been successfully used for static embeddings. We now propose to apply it to contextualized representations. Given a gendered word list $V:=\{v_1,v_2,\dots,v_n\}$, we feed sentences which contain the gendered words into BERT without applying any masking. Due to the different contexts, for each word we regard every occurrence in a corpus $v_{i_j}$ with the corresponding embedding $e_{i_j}$. For each sentence that contains the word $v_{i_j}$, the output of each BERT layer yields a contextual embedding $e_{i_j}^m,m\in\{1,2,\dots,M\}$, where $M$ is the number of layers in the BERT model. We compute an orthogonal rotation matrix $Q_m$ as given in formula \ref{eq:densray2}. \enote{pd}{I a m missing some context here, it's not fully clear which vectors you use for DensRay...}

We believe that by identifying the interpretable gender subspace and replacing its parameters by $0$, the gender information that may cause bias can be mitigated. When a new sentence or corpus is sent into the inference process of BERT, we can rotate its original embedding on the output of the first BERT layer $E_1$ by the pre-computed $Q_m$ to an interpretable embedding $\hat{E_1}:=E_1Q_1$. Then we replace the first dimension of $\hat{E_1}$ by $0$ and rotate it back to get a new $E_1:=\hat{E_1}Q_1^T$, since $Q^{-1}=Q^T$ for orthogonal matrix $Q$. This operation can be applied after every layer in the inference process to yield a debiased BERT output. \enote{pd}{the above is just how you implemented it -- I do not think it is very relevant for the paper. The main message is, that you identify the gender dimensions and then you remove them by zeroing them out, right?}

\subsection{Evaluations}\label{sec:eval}
In this paper we will use WEAT to measure gender bias. Besides, to quickly test the results of debiasing, we also constructed a set of templates with 320 occupation names\footnote{https://github.com/tolga-b/debiaswe/blob/master/data/professions.json} provided by \citet{bolukbasi2016man} for masked language modeling: "[MASK] is a \textit{occupation}." The gender bias in the templates is measured by the average difference between the probability of BERT predicts [MASK] as "he" and "she",
\begin{eqnarray}
    diff=\mathop{mean}_{i \in templates}(\mathop{prob}(he)-\mathop{prob}(she))\nonumber
\end{eqnarray}
\enote{pd}{maybe use nicer names for the formula (at least like \text{diff}}
Through case study, we find that for most templates the probability of "he" is higher than "she", which qualitatively indicates that gender bias exists in these templates. We also find that for most sentences the sum of the two probability is higher than 0.7, which means that the predictions will be stable. This templates set can be easily extended to other languages that do not have gendered profession names, like Chinese or Persian.
