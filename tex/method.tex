\subsection{DensRay debiasing on BERT}
While DensRay has been successfully used for static embeddings. We now propose to apply it to contextualized representations. Given a gendered word list $V:=\{v_1,v_2,\dots,v_n\}$, we feed sentences which contain the gendered words into BERT without applying any masking. Due to the different contexts, for each word we regard every occurrence in a corpus as $v_{i_j}$, with the corresponding embedding $e_{i_j}$. For each sentence that contains the word $v_{i_j}$, the output of each BERT layer yields a contextual embedding $e_{i_j}^m,m\in\{1,2,\dots,M\}$, where $M$ is the number of layers in the BERT model. We compute an orthogonal rotation matrix $Q_m$ for the $m$th BERT layer as given in formula \ref{eq:densray2}. We believe that by replacing parameters in the gender subspace by $0$, the gender information that may cause bias can be mitigated. In this paper, we take the first dimension of the rotated space as the gender subspace.

\subsection{Evaluations}\label{sec:eval}
In this paper we will use WEAT to measure gender bias. Besides, to quickly test the results of debiasing, we also constructed a set of templates with 320 occupation names\footnote{https://github.com/tolga-b/debiaswe/blob/master/data/professions.json} provided by \citet{bolukbasi2016man} for masked language modeling: "[MASK] is a \textit{occupation}." The gender bias in the templates is measured by the average difference between the probability of BERT predicts [MASK] as "he" and "she",
\begin{eqnarray}
    \text{diff}=\mathop{mean}_{i \in templates}(\mathop{prob}(he)-\mathop{prob}(she))\nonumber
\end{eqnarray}
Through case study, we find that for most templates the probability of "he" is higher than "she", which qualitatively indicates that gender bias exists in these templates. We also find that for most sentences the sum of the two probability is higher than 0.7, which means that the predictions will be stable. This templates set can be easily extended to other languages that do not have gendered profession names, like Chinese or Persian.
