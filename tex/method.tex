
\enote{hs}{standardize: either pretrained langauge model or
  contextualized langauge model. (i ugess contextualized is better)}

\subsection{Adapting DensRay to Contextualized Language Models}
We now describe how we adapt DensRay to contextualized
language models. Given a set of gendered words
$V$, we extract sentences containing a word in $V$ from a
corpus. We run a contextualized language model
with $M$ layers
on each
sentence
$t_1,\ldots,t_j,\ldots,t_n$ (where $t_j \in V$)
and compute the contextualized representations $e^m, 1\leq m
\leq M$ of $t_j$, one for each layer. 
We compute an orthogonal rotation
matrix $Q_m$ for the $m$th BERT layer using Eq.\
\ref{eq:densray2}.
Finally, for debiasing, we set the dimensions
of the gender subspace to $0$ with the goal of eliminating or at least reducing gender
information that may cause bias; for measuring bias, we use the distance to the zero point of the gender subspace as the measurement. In this paper, we take the first dimension of the rotated space as the gender subspace.

\subsection{Evaluation}\label{sec:eval}
We use two evaluation datasets
to measure gender
bias: WEAT (Section~\ref{sec:weat}) and OCCTMP.

OCCTMP is a new evaluation dataset
based on occupation templates
that we created specifically
for
the evaluation of contextualized language models.  It has
the added advantage that results are easier to interpret
than those for WEAT.

To construct OCCTMP,
we start with 
320 occupation
names\footnote{https://github.com/tolga-b/debiaswe/blob/master/data/professions.json}
provided by \citet{bolukbasi2016man}.
Each occupation name is converted into a template of the form
``[MASK] is an \textit{occupation}.''
We measure
gender bias
in the templates as the average difference
between the probability of BERT predicting [MASK] as ``he''
vs.\ ``she''
\begin{eqnarray}
    \text{diff}=\frac{1}{|{\mathcal T}|} \sum_{T \in
      {\mathcal T}}(p(\mbox{he}| T) - p(\mbox{she}|T))\nonumber
\end{eqnarray}
where $\mathcal T$ is the set of 320 templates.  We find
that for most experiments and most templates the probability
of ``he'' is higher than ``she'', which qualitatively
indicates that gender bias can be identified using these templates. We
also find that in most cases the sum of the two probability
is higher than 0.7; thus, this evaluation task is a good fit
for BERT because it has learned that a pronoun is likely to
occur in the masked position.
Our templates  can be easily extended to other
languages
as we later show for
Chinese.
