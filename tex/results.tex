
\subsection{Debiasing Results}
\tabref{t:templates1} gives results on OCCTMP. Two OCCTMP
examples are given in \tabref{t:templates2}. We see that
DensRay can mitigate the gender bias in BERT as measured by
diff: bias between predicting he/she drops by large margin
(e.g., for bert-base from 1.98 to 0.36). The table indicates
that DensRay outperforms the other two methods on OCCTMP.
Note that
the prediction probabilities of debiasing conceptor are
quite low for both ``he'' and ``she'' indicating that conceptor debiasing might affect language model performance. However,
in relative terms, ``he'' is still strongly favored compared
to ``she''.
\tabref{t:weat1} shows the results on association tests. We observe that association tests have a high variance depending on which category is used. 
Overall the debiasing performance of the three methods are comparable with DensRay and Conceptor having the best performance three times and hard-debiasing having the best performance 5 times.
Overall the debiasing performance of the three methods are comparable, with DensRay and Conceptor both having the best performance three times and hard-debiasing having the best performance five times.

\begin{table}[ht]
\vspace{-5pt}
\centering
\footnotesize
\setlength{\abovecaptionskip}{0.2cm}   
\setlength{\belowcaptionskip}{0cm}
\begin{tabular}{llccccc}
	\toprule
	model & debiasing &$p(\text{he} | {\mathcal T})$ & $p(\text{she} | {\mathcal T})$& sum & diff & std\\
	\midrule
	\multirow{4}{2cm}{bert-base} & - & 0.66 & 0.19 & 0.85 &1.98&1.39\\
	&hard & 0.35 & 0.42 & 0.77&0.42&0.09\\
	&conceptor & 0.18 & 0.11 & 0.28 & 0.68&0.26\\
	&densray & 0.48 & 0.37 & 0.86&\textbf{0.36}&0.07\\
	\midrule
	\multirow{4}{2cm}{bert-large} & -  & 0.63 & 0.19 & 0.82  &1.82&1.30\\
	&hard & 0.40 & 0.23 & 0.63&0.69&0.30\\
	&conceptor & 0.43 & 0.18 & 0.61 & 1.03&0.53\\
	&densray  & 0.47 & 0.31 & 0.77&\textbf{0.49}&0.13 \\
	\bottomrule
\end{tabular}
\caption{\tablabel{t:templates1} BERT debiasing results on OCCTMP, measure by \textit{diff}, \textit{std}, and \textit{sum} described in \ref{sec:occtmp}}

\vspace{0.4cm}

	\begin{tabular}{cl{|}cccc{|}cccc}
	\bottomrule
	&&\multicolumn{4}{c|}{bert-base}&\multicolumn{4}{c}{bert-large}\\
	%\hline
	&&\multicolumn{2}{c}{WEAT}&\multicolumn{2}{c|}{SEAT}&\multicolumn{2}{c}{WEAT}&\multicolumn{2}{c}{SEAT}\\
	\hline
	category & debiasing & $d$ & $p$& $d$ & $p$& $d$ & $p$& $d$ & $p$\\
	\hline
	\multirow{4}{*}{C6} & - & 0.66 & 0.08 &1.04&$<$10$^{-2*}$& 1.57 & $<$10$^{-2*}$ &0.50&$<$10$^{-2*}$\\
	& hard& 0.15 & 0.38&\textbf{-0.08}&0.67& 0.80 & 0.06&0.07&0.35\\
	& conceptor & \textbf{0.07} & 0.46&0.77&$<$10$^{-2*}$ & 1.33 & $<$10$^{-2*}$&\textbf{0.06}&0.37\\
	&densray & 0.62 & 0.12&0.36&0.02$^{*}$ & \textbf{0.76} & 0.07&0.13&0.22\\
	\hline
	\multirow{4}{*}{C7} &- & 0.60 & 0.11 &0.17&0.15 & -0.40 & 0.75 &0.38&0.01$^{*}$\\
	& hard& \textbf{-0.07} & 0.56&\textbf{-0.06}&0.64 & -0.51 & 0.83&0.38&0.01$^{*}$\\
	& conceptor & 0.54 & 0.14&-0.25&0.93 & -0.32 & 0.73&\textbf{-0.60}&0.99\\
	& densray & 0.09 & 0.45&-0.47&0.99 & \textbf{0.06} & 0.05&-0.73&0.99\\
	\hline
	\multirow{4}{*}{C8}& - & 0.78 & 0.08 &0.81&$<$10$^{-2*}$ & -0.60 & 0.87 &-0.30&0.95\\
	& hard & -0.29 & 0.68&\textbf{-0.10}&0.71 & 0.78 & 0.06&\textbf{-0.03}&0.56\\
	& conceptor & 0.62 & 0.14&0.50&$<$10$^{-2*}$& \textbf{0.12} & 0.39&0.30&0.94\\
	& densray & \textbf{0.03} & 0.47&0.41&0.01$^{*}$ & 0.20 & 0.33&-0.66&0.99\\
	\toprule
\end{tabular}
\caption{\tablabel{t:weat1}
	BERT debiasing results on association tests. * shows significant gender bias. Only models without significant gender bias ($p>0.05$) can be accepted.}
\end{table}

\begin{table}[h]
	\centering
	\footnotesize
	\vspace{0cm}  
	\setlength{\abovecaptionskip}{0.0cm}   
	\setlength{\belowcaptionskip}{0cm}

\end{table}

\subsection{Model Performance}
\tabref{t:glue1} shows that DensRay debiasing gets comparable results with
the original models on Wikitext-2 and GLUE tasks.
In most tasks on bert-base and all tasks on bert-large, DensRay performs better than hard debiasing, so DensRay affects model performance less.
Similarly, in most tasks on bert-base and all tasks but one
on bert-large, DensRay performs better than debiasing conceptor. Overall we find that DensRay affects model performance the least among the considered methods.
\begin{table*}[h]
\centering
\footnotesize
\vspace{-5pt}  
\setlength{\abovecaptionskip}{0.2cm}   
\setlength{\belowcaptionskip}{0cm}
\begin{tabular}{c|l||c|cccccc|c}
%\hline
model&debiasing & Wikitext-2&CoLA &SST-2&MRPC&STS-B&RTE&WNLI&GLUE avg\\
\hline\hline
\multirow{4}{*}{bert-base}&- &3.77&49.15&92.09&85.86&82.66&62.82&52.11&70.78\\
&hard &3.95&45.53&\textbf{91.74}&82.48&\textbf{82.60}&63.54&\textbf{56.34}&70.37\\
&conceptor &4.46&\textbf{48.31}&91.43&84.08&81.37&59.57&\textbf{56.34}&70.18\\
&densray &\textbf{3.81}&48.04&\textbf{91.74}&\textbf{84.89}&82.43&\textbf{63.90}&53.52&\textbf{70.75}\\
\hline
\multirow{4}{*}{bert-large}&- 
&3.29& 47.93&94.90&89.30&87.60&70.10&65.10&75.82\\
&hard &3.85& 47.45&93.95&85.01&82.33&67.12&63.02&73.15\\
&conceptor &4.13&\textbf{49.44}&93.87&87.67&83.44&62.45&56.34&72.20\\
&densray &\textbf{3.35}& 48.91&\textbf{94.02}&\textbf{88.84}&\textbf{85.63}&\textbf{67.78}&\textbf{64.48}&\textbf{74.94}\\
%\hline
\end{tabular}
\caption{\tablabel{t:glue1}
Language modeling perplexity and GLUE tasks
performance. }
\end{table*}


\subsection{Examples}
In \tabref{t:templates2} we show two OCCTMP examples.
The sum probabilities of ``he'' and ``she''
on the debiasing conceptor are around 0.5, indicating that
the language model has lost part of its ability to predict that 
a pronoun is  likely to occur in the masked position.
\begin{table}[h]
	\centering
	\footnotesize
	\vspace{-5pt}  
	\setlength{\abovecaptionskip}{0.2cm}   
	\setlength{\belowcaptionskip}{0cm}  
	\begin{tabular}{llcccc}
		\toprule
		template $T$ & debiasing & $p(he|T)$ & $p(she|T)$ &sum&diff\\
		\midrule
		 & - & 0.84 & 0.13&0.97&1.86\\
		\text{[MASK]} is a& hard& 0.37 & 0.55&0.92&0.40\\
	    professor.	& conceptor& 0.28 & 0.23&0.51&{0.20}\\
		& densray & 0.53 & 0.37&0.90&0.36\\
		\midrule
		 & - & 0.22 & 0.72&0.94&1.19\\
		\text{[MASK]} is a& hard& 0.27 & 0.64&0.91&0.86\\
		dancer.  & conceptor& 0.20 & 0.33&0.53&0.50\\
		& densray& 0.42 & 0.52&0.94&0.21\\
		\bottomrule
	\end{tabular}
	\caption{\tablabel{t:templates2}
		OCCTMP examples with prediction probabilities (model:bert-base).}
\end{table}

\subsection{Analysis}

\subsubsection*{Debiasing on Attention Heads and Layers}
We now apply DensRay to single attention heads in BERT and investigate the debiasing effect on OCCTMP. The heatmap \figref{fig:heads} shows that the debiasing effect of one single attention head is not apparent, with diff scores all in (1.0,1.4). We conjecture that there is no single attention head that is responsible for processing gender information.
This conjecture is similar to the experiment from \newcite{bau2019identifying}, which showed modifying some neurons activations in NMT did not help in controlling gender, and speculated that gender property is very distributed on neurons.

So far we have always applied DensRay to all BERT layers
simultaneously. \figref{fig:layersbase}  illustrates the effect of
debiasing a single  layer on OCCTMP and the three
WEAT categories. In contrast to the attention heads we observe a different debiasing effect across different layers. We see that the debiasing effect is
stronger in layers 7--10 than in the other layers in
bert-base. This indicates that gender information is processed on BERT layers, especially the upper layers.
\begin{figure}[h]
	\centering
	\footnotesize
	\vspace{-5pt}  
	\setlength{\abovecaptionskip}{0.1cm}   
	\setlength{\belowcaptionskip}{0cm}
	\subfigure[attention heads]{
		\begin{minipage}[l]{0.5\linewidth}
			\centering
			\includegraphics[width=0.75\linewidth]{heads}
			\figlabel{fig:heads}
		\end{minipage}%
	}%
	\subfigure[layers]{
		\begin{minipage}[r]{0.5\linewidth}
			\centering
			\includegraphics[width=0.75\linewidth]{layers}
			\figlabel{fig:layersbase}
		\end{minipage}%
	}%
	\centering
	\caption{(a): DensRay debiasing on each single attention head in bert-base, measured by \text{diff} on OCCTMP. (b): DensRay debiasing on each single layer, measured by \text{diff} on  OCCTMP and $d$-value on WEAT.}
	\figlabel{fig:headsandlayers}
\end{figure}

\subsubsection*{Quantifying Gender Bias with DensRay}
\seclabel{quantify}
DensRay can be used to quantify gender bias for sentences
and tokens. We use the distance to the origin in gender
subspace as the measure. In BERT, we use the average
bias score of tokens to quantify the whole
sentence. \tabref{t:measure1} compares DensRay with the log
probability score \cite{kurita2019measuring}, which
quantifies gender bias based on templates of form ``[TARGET]
is a [ATTRIBUTE]''. We regard zero as a balance point
without bias. Contrary to the log probability score, a
positive DensRay score represents the level of female
bias. These examples show that DensRay is more versatile, it
can quantify  bias both on the token and on the sentence level in contrast to the log-probability score.
In the sentence ``The professor asked the nurse .'' one can immediately see that the model has a male bias on ``professor'' and a female bias on ``nurse'', although the sentence itself
is completely gender-neutral.
\begin{table}[h]
	\centering
	\footnotesize
	\vspace{-5pt}  
	\setlength{\abovecaptionskip}{0.1cm}   
	\setlength{\belowcaptionskip}{0cm}
	\begin{tabular}{l|c||c}
	\bottomrule
	\hspace{1.4cm}DensRay&Avg.&log probability score\\
	\hline
	%\hline
	%\specialrule{.05em}{.1ex}{.65ex}
	\hlc[pink!4]{[MASK]} \hlc[pink!18]{cooked} \hlc[pink!8]{dinner} \hlc[cyan!2]{.}
	&\hlc[pink!7]{0.14}& -1.04\\[2pt]
	%\hline
	\hlc[cyan!34]{[MASK]} \hlc[cyan!48]{is} \hlc[cyan!45]{a} \hlc[cyan!7]{professor} \hlc[pink!22]{.}
	&\hlc[cyan!34]{-0.45}& 0.63\\[2pt]
	%\hline
	\hlc[pink!100]{[MASK]} \hlc[pink!67]{is} \hlc[pink!85]{a} \hlc[pink!96]{nurse} \hlc[pink!25]{.}
	&\hlc[pink!79]{1.58}& -5.44\\[2pt]
	%\hline
	\hlc[cyan!62]{The} \hlc[cyan!27]{professor} \hlc[cyan!4]{asked} \hlc[pink!29]{me} \hlc[pink!17]{.}
	&\hlc[cyan!10]{-0.19} & not applicable\\[2pt]
	%\hline
	\hlc[cyan!65]{The} \hlc[cyan!12]{professor} \hlc[pink!12]{asked} \hlc[pink!64]{the} \hlc[pink!100]{nurse} \hlc[pink!22]{.}
	&\hlc[pink!22]{0.43} &not applicable\\[2pt]
	%\hline
	\hlc[cyan!38]{The} \hlc[pink!9]{child} \hlc[cyan!0]{played} \hlc[pink!7]{with} \hlc[pink!11]{the} \hlc[pink!28]{car} \hlc[cyan!6]{.}
	&\hlc[pink!2]{0.03} &not applicable\\[2pt]
	%\hline
	\hlc[cyan!9]{The} \hlc[pink!32]{child} \hlc[pink!30]{played} \hlc[pink!28]{with} \hlc[pink!47]{the} \hlc[pink!53]{doll} \hlc[cyan!11]{.}
	&\hlc[pink!25]{0.49}&not applicable\\
	\toprule
	\end{tabular}
	\caption{\tablabel{t:measure1}
		Examples for quantifying bias (model: bert-base), where red(blue) denotes female(male) bias.}
\end{table}




\subsubsection*{Alternatives to Removing Gender Bias}
In our experiments, we zero out the dimensions of the gender
subspace to remove gender bias. 
We also explored three alternatives to zeroing out. 1)
Replace the first dimension of the gender subspace with the
mean value of the first dimension of the training
samples. 2) Standardize the first dimension. 3) Replace the
first dimension with a small random variable sampled from a Gaussian distribution. All of them did not perform well. Using higher dimensional gender subspaces also did not improve the debiasing results while harming the model performance.

When debiasing is applied to all layers in our experiments, we computed all the rotation matrices from the original model in advanced, and then successively applied them to the corresponding layer in the forward pass. However the gender direction on layer $l_{i+1}$ will be changed after debiased on $l_i$. Our approach is a compromise for faster training, by only focus on the original gender direction. A better alternative may be to computed a new rotation matrix for $l_{i+1}$ after debiased on $l_i$.


\subsection{Multilingual Debiasing}
We now show that, in a multilingual contextualized language model like mBERT, we can use DensRay for zero-shot debiasing. Specifically, we train a DensRay model on English and use it to debias Chinese.

In our opinion, multilingual debiasing involves two distinct
	problems. First, how can bias be removed from the underlying
	model? Second, how does bias manifest in a particular
	language? The removal of bias from the underlying model can
	be argued to be largely independent of the language whereas
	the way bias manifests is highly language-dependent. For
	example, Chinese does not mark gender and most German nouns
	describing people are gender-specific. So on the surface,
	Chinese are gender-neutral and German
	cannot be gender-neutral (e.g., \textit{Studentinnen} and \textit{Studenten}). However, these particularities of surface form of individual
	languages do not change the underlying problem of biased
	language models: Chinese and German language models are
	still biased and should be debiased to avoid unfair and
	biased impact caused by deployed NLP systems.
	
We use  bert-base-multilingual-uncased from
\cite{wolf2019huggingfaces} with the same setup as for
bert-base-uncased in our previous experiments. As before, we
compute the rotation matrices using the English gendered
words from the ``family'' category of the Google analogy
test set \cite{mikolov2013efficient}. Since Chinese is a language that does not mark gender, we construct OCCTMP by directly translating from the English templates. We use the following form:
``\text{[MASK]}\yin{是一个}\textit{occupation}\yin{。}'' We translate the occupations using Tencent Translation\footnote{\url{fanyi.qq.com/}} and make some manual adjustments to the translation. After removing duplicates (e.g. firefighter and fireman have the same translation is Chinese), 302 Chinese templates remain.

\tabref{t:templates3} gives results for the Chinese templates. Two examples are given in \tabref{t:templates5}. We see that DensRay trained with English can mitigate gender bias in mBERT: the diff score drops from 1.39 to 1.22 on Chinese templates. 
\begin{table}[h]
	\centering
	\footnotesize
	\vspace{-6pt}  
	\setlength{\abovecaptionskip}{0.1cm}   
	\setlength{\belowcaptionskip}{0cm}
	\begin{tabular}{lccccc}
		\toprule
		debiasing & $p(he|\mathcal{T}$) & $p(she|\mathcal{T}$) & sum &diff & std\\
		\midrule
		 -en 
		& 0.51 & 0.14 & 0.65 & 1.66&0.90 \\ 
		densray-en & 0.33 & 0.12 & 0.46 & 1.33&0.81 \\
		\midrule
		 -zh 
		& 0.24 & 0.07 & 0.31 & 1.39&0.77 \\
		 densray-zh 
		& 0.12 & 0.04 & 0.17 & 1.22&0.69\\
		\bottomrule
	\end{tabular}
	\caption{\tablabel{t:templates3}
		Results of OCCTMP on mBERT after applied DensRay. \textit{-en} denotes we test on English templates, and textit{-zh} on Chinese templates.}
	
\vspace{0.4cm}

	\begin{tabular}{llcccc}
	\toprule
	template $T$ & debiasing & \yin{$p$(他$|T)$} & \yin{$p$(她$|T)$}&sum&diff\\
	\midrule
	& -en & 0.68 & 0.16&0.84&1.45\\
	\yin{\text{[MASK]}是一个客座教授。}
	&densray-en & 0.51 & 0.18&0.70&1.04\\
	\text{[MASK]} is an adjunct professor.
	& -zh & 0.52 & 0.11&0.63&1.55\\
	& densray-zh & 0.30 & 0.08&0.38&1.31\\
	\midrule
	& -en & 0.53 & 0.17&0.70&1.14\\
	\yin{\text{[MASK]}是一个管理员。}  
	&densray-en & 0.35 & 0.13&0.48&0.99\\
	\text{[MASK]}is an administrator. 
	& -zh & 0.68 & 0.16&0.84&1.45\\
	& densray-zh & 0.51 & 0.18&0.69&1.04\\
	\bottomrule
\end{tabular}
\caption{\tablabel{t:templates5}
	Sanity check on the Chinese templates, where \yin{\textit{他}} means \textit{he} and \yin{\textit{她}} means \textit{she}. Corresponding English translations are shown below the Chinese.}
\end{table}

