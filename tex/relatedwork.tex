\subsection{Quantifying Gender Bias}
A typical way to measure gender bias is to evaluate on
\textbf{downstream tasks}. For coreference resolution,
\cite{zhao2018gender} designed Winobias and
\cite{rudinger2018gender} designed Winogender schemas. In
contrast to WinoBias, Winogender schemas include
gender-neutral pronouns. One Winogender schema has one
occupational mention and one ``other participant'' mention
while WinoBias has two occupational mentions. \enote{pd}{is
  the difference between Winobias and Winogender relevant to
  this work?}\cite{webster2018mind} released GAP, a
balanced corpus of Gendered Ambiguous Pronouns, which
measures gender bias as the ratio of F1 score on masculine
to F1 score on feminine. However the ratio is very close to 1 \shortcite{Chada_2019, Attree_2019} making it hard to compare debiasing systems. For sentiment analysis, Equity Evaluation Corpus (EEC) \shortcite{Kiritchenko_2018} was designed to measure gender bias by the difference in emotional intensity predictions between gender-swapped sentences.

An alternative way to measure gender bias is based on \textbf{association tests}, which originated from sociological research. \cite{greenwald1998measuring} proposed the Implicit Association Test (IAT) to quantify societal bias. In the IAT, response times were recorded when subjects were asked to match two concepts. For example, subjects were asked to match black and white names with ``pleasant'' and ``unpleasant'' words. Subjects tended to have shorter response times for concepts they thought associated. Based on the IAT, \cite{caliskan2017semantics} proposed the Word Embedding Association Test (WEAT), which uses word similarities between targets and attributes instead of the response times to get rid of the requirement of human subjects. \cite{may2019measuring} extended WEAT to the Sentence Embedding Association Test (SEAT); \cite{kurita2019measuring} proposed a template-based log probability bias score to measure the association between targets and attributes in BERT.

\enote{hs}{for many of the papers you discuss above it's not
  clear what the realtinship to the current work is. this
  hsould always be clear}


\subsection{Debiasing Methods}
 Many methods to remove gender bias have been proposed. The
 most common way is to define a gender direction (or, more
 generally, a subspace) by a set of gendered words, and
 debias the word embeddings in a post-processing
 projection. \cite{bolukbasi2016man} propose (i) \emph{hard
   debiasing}: they use the gendered words to compute the
 difference embedding vector as the gender direction; and
 (ii) \emph{soft debiasing},
 a
 machine learning based method
that combines
 the inner-products objective of word embedding and an
 objective to project the word embedding into an orthogonal
 gender subspace. Hard debiasing has been found to work
 better. \enote{pd}{should we mention hard-debiasing by mu
   et al here and explain the difference to bolukbasi?}
 \cite{dev2019attenuating} explored partial projection and
 some simple tricks to improve the hard debiasing
 method. \cite{zhao2019gender} applied the data
 augmentation and debiasing method of
 \cite{bolukbasi2016man} to mitigate gender bias on ELMo
 \shortcite{Peters:2018}. \cite{karve2019conceptor} introduce
 the debiasing conceptor: they shrink each
 principal component of the covariance matrix of the word
 embeddings to achieve a soft debiasing. Besides the above
 post-processing methods, \shortcite{zhao2018learning} propose
 GN-Glove: it debiases during training to learn word
 embeddings with protected attributes. The method we use
 here, DensRay, is similar to
hard debiasing in that we find
and eliminate a gender subspace in post-processing.
But DensRay can be solved efficiently in closed form and it
is more stable than hard debiasing.

 



